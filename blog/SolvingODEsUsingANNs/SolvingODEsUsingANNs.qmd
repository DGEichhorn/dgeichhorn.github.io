---
title: "Post titel: ODE via NN"
description: "Implementing from scratch a neural network based approach for approximating the solutions of ordinary differential equations."
#author: "Author"
date: "5/22/2021"
#image: "cover.jpg"
categories:
  - Deep Learning
  - Python
  - Differential Equations
---

<!--% Back up ODE: torch.mean( (dy_dx - (x+y)/x)**2 ) + (y[0] + 0.1)**2

# To Dos:
- ACHTUNG: evtl noch randomness in NN training (diese ausmerzen) UND ODER EINEN ANDEREN SEED VERWENDEN
- nach zweiter LossODE: HIDE code show plots
  - construct model, criterion and loss
  - training
  - visualization with analyticaly solution
-->

# Introduction

The impressive capabilities of neural networks in typical regression and classification tasks are widely known. Recently, I read of a lesser known but mathematically very interesting application of neural networks. In fact, neural networks, can be used to numerically approximate the solutions of (ordinary) differential equations. Before implementing it from scratch in Python, I will first describe the setting and the general approach.

# Setting

Let's first introduce some notation: $u(x)$ denotes an unknown function, $u: I \subseteq \mathbb{R} \to \mathbb{R}$, of the independent variable $x$. The $j$-th derivative of $u$ evaluated at $x$ is written as:
$$u^{(j)}(x)=\frac{d^j \, u}{d \, x^j}(x), \; j \in \mathbb{N}.$$
An ordinary differential equation (ODE) of order $k$ is an equation of the following form:
$$F \left( x, u(x), u^{(1)}(x), \dots, u^{(k)}(x) \right) = 0, \; \forall x \in I,$$
where $F$ is a given function, $F: I \times  \mathbb{R}^{k+1} \to \mathbb{R}$. So solving an ODE amounts to finding a function $u$ which satisfies the relationship specified by $F$ between $u$ and its first $k$ derivatives, as well as $x$. Under certain assumptions (see Picard-Lindelöf theorem), for such an ODE a unique solution is guaranteed to exist for given initial conditions. These initial conditions specify the value of the unknown function and its first $k-1$ derivatives at a certain initial point $x_0 \in I$, i.e. something like
$$u(x_0)=u_0, \; u^{(1)}(x_0)=u_1, \; \dots, \; u^{(k-1)}(x_0)=u_{k-1}.$$
While some ODEs can be solved analytically, in most cases the solution needs must be approximated numerically. 
For this purpose, neural networks can be utilized, although the setting differs largely from the typical supervised learning setting.
<!--A classical numerical procedure for finding approximate solutions is the Runge-Kutta method.-->

# General Approach

Let $N_{\theta}(x)$ denote a feed forward multilayer perceptron parametrized by $\theta$ (i.e. a vector containing all weights and biases), $N_{\theta}: I \to \mathbb{R}$. If the activation functions employed by $N_{\theta}$ are (infinitely) differentiable, $N_{\theta}$ is -- as a composition of (infinitely) differentiable functions -- itself (infinitely) differentiable. Examples of (infinitely) differentiable activation functions are $tanh$ and $sigmoid$. Their infinite differentiability is the crucial property that makes feed forward multilayer perceptrons unlike other highly flexible function approximators particularly well suited for representing solutions to ODEs.
<!--Infinite differentiability sets feed forward multilayer perceptrons apart from other highly flexible function approximator such as random forests and makes them suitable for the task of solving ODEs.-->
At any given $x$, the value of the derivatives can be computed using backpropagation.


IDEA: sample eine menge von punkten in interval indem wir IVP lösen wollen; replace die unbekannte funktion und deren ableitungen in ODE durch N(x) und ihre ableitungen; passe die Parameter von N(x) so an dass F(x, N(x),...) = 0 ist; das ist gleichbedeutend zu


replace u(x) by N(x)





# Implementation

WICHTIG: für die beiden unten behandelten ODEs gibt es bekannte analytical solutions WICHTIG DAMIT WIR DIE TRUE ANALYTICAL SOLUTION MIT DEM VOM NN ERLERNTEN VERGLEICHEN KÖNNEN

werden zunächst die allgemeine idee grob umreißen und dann für zwei spezielle odes from scratch coden wie man mit NN diese numerisch lösen kann

# Load Packages

```{python}
# for constructing, learning, using NNs
import torch
import torch.nn as nn 
import torch.optim as optim

# for visualization
import matplotlib.pyplot as plt
```

# set seed for reproducibility

```{python}
seed = 42
torch.manual_seed(seed)
```

# Define Neural Network

```{python}
class Network(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(Network, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Sigmoid(),
      nn.Linear(hidden_size, output_size)
      )
    
  def forward(self, x):
    return self.net(x)

```

# First ODE

## Define Custom Loss Function

```{python}
class LossODE(nn.Module):
  def __init__(self):
    super(LossODE, self).__init__()
  
  def forward(self, model, x):
    x = x.clone().detach().requires_grad_(True)
    y = model(x)
    
    dy_dx = torch.autograd.grad(
      outputs=y,
      inputs=x,
      grad_outputs=torch.ones_like(y),
      create_graph=True
      )[0]
    
    d2y_dx2 = torch.autograd.grad(
      outputs=dy_dx,
      inputs=x,
      grad_outputs=torch.ones_like(dy_dx),
      create_graph=True)[0]
    
    loss_DE = torch.mean((d2y_dx2 + 1*dy_dx + 4*y)**2)
    loss_initial = (y[0] - 0.5)**2 + (dy_dx[0] - 2)**2
    loss_total = loss_DE + loss_initial
    
    return loss_total

```

## Construct model, loss and optimizer

```{python}
model = Network(input_size=1, hidden_size=10, output_size=1)
criterion = LossODE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

## Set x-values

```{python}
x = torch.linspace(0, 5, 100)[:, None]
```

## Training

WICHTIG: Kommentar auch in ANN für ODEs bzgl. epochs & PRINT REPLACE ÜBERNEHMEN

```{python}
num_epochs = 10
steps_per_epoch = 3000

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    loss = criterion(model, x)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

```
## Visualize learned and analytical solution

```{python}
#| code-fold: true
#| code-summary: "Show code"

x = torch.linspace(0, 15, 1000)[:, None]
with torch.no_grad():
  y = model(x)

plt.figure(figsize=(6, 5))
plt.plot(x, y, color="C3", label="Neural Network")
plt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), color="C0", label="Analytical", linestyle="dotted")
plt.title("Solution of the ODE")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.grid()
plt.show()
```

# Second ODE

## Define Custom Loss Function

```{python}
class LossODE(nn.Module):
  def __init__(self):
    super(LossODE, self).__init__()
  
  def forward(self, model, x):
    x = x.clone().detach().requires_grad_(True)
    y = model(x)
    
    dy_dx = torch.autograd.grad(
      outputs=y,
      inputs=x,
      grad_outputs=torch.ones_like(y),
      create_graph=True
      )[0]
    
    loss_DE = torch.mean((dy_dx - 2*x*(2-y))**2)
    loss_initial = (y[0] + 1)**2
    loss_total = loss_DE + loss_initial
    
    return loss_total

```

## set seed for reproducibility

```{python}
seed = 42
torch.manual_seed(seed)
```


## Construct model, loss and optimizer

```{python}
model = Network(input_size=1, hidden_size=10, output_size=1)
criterion = LossODE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

## Set x-values

```{python}
x = torch.linspace(0, 5, 100)[:, None]
```

## Training

```{python}
seed = 42
torch.manual_seed(seed)

num_epochs = 10
steps_per_epoch = 3000

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    loss = criterion(model, x)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

```
# Visualize learned and analytical solution

```{python, fig.align=center}
#| code-fold: true
#| code-summary: "Show code"

x = torch.linspace(-5, 5, 1000)[:, None]
with torch.no_grad():
  y = model(x)

plt.figure(figsize=(6, 5))
plt.plot(x, y, color="C3", label="Neural Network")
plt.plot(x, 2 - 3*torch.exp(-x**2), color="C0" , label="Analytical", linestyle="dotted")
plt.title("Solution of the ODE")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.grid()
plt.show()
```