% Back up ODE: torch.mean( (dy_dx - (x+y)/x)**2 ) + (y[0] + 0.1)**2

# To Dos:
- nach zweiter LossODE: HIDE code show plots
  - construct model, criterion and loss
  - training
  - visualization with analyticaly solution

# Load Packages

```{python}
import torch                        # for constructing, learning, using NNs
import torch.nn as nn 
import torch.optim as optim

import matplotlib.pyplot as plt     # for visualization
```

# Define Neural Network

```{python}
class Network(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(Network, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Sigmoid(),
      nn.Linear(hidden_size, output_size)
      )
    
  def forward(self, x):
    return self.net(x)

```

# Define Custom Loss Function

```{python}
class LossODE(nn.Module):
  def __init__(self):
    super(LossODE, self).__init__()
  
  def forward(self, model, x):
    x = x.clone().detach().requires_grad_(True)
    y = model(x)
    
    dy_dx = torch.autograd.grad(
      outputs=y,
      inputs=x,
      grad_outputs=torch.ones_like(y),
      create_graph=True
      )[0]
    
    d2y_dx2 = torch.autograd.grad(
      outputs=dy_dx,
      inputs=x,
      grad_outputs=torch.ones_like(dy_dx),
      create_graph=True)[0]
    
    loss_DE = torch.mean((d2y_dx2 + 1*dy_dx + 4*y)**2)
    loss_initial = (y[0] - 0.5)**2 + (dy_dx[0] - 2)**2
    loss_total = loss_DE + loss_initial
    
    return loss_total

```

# Construct model, loss and optimizer

```{python}
model = Network(input_size=1, hidden_size=10, output_size=1)
criterion = LossODE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

# Set x-values

```{python}
x = torch.linspace(0, 5, 100)[:, None]
```

# Training

```{python}
num_epochs = 50
steps_per_epoch = 1000

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    loss = criterion(model, x)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

```
# VISUALIZE SOLUTION

```{python}
x = torch.linspace(0, 15, 100)[:, None]
with torch.no_grad():
  y = model(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, label="Predicted")
plt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), '--', label="Analytical")
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.grid()
plt.show()
```

```{python}
class LossODE(nn.Module):
  def __init__(self):
    super(LossODE, self).__init__()
  
  def forward(self, model, x):
    x = x.clone().detach().requires_grad_(True)
    y = model(x)
    
    dy_dx = torch.autograd.grad(
      outputs=y,
      inputs=x,
      grad_outputs=torch.ones_like(y),
      create_graph=True
      )[0]
    
    loss_DE = torch.mean((dy_dx - 2*x*(2-y))**2)
    loss_initial = (y[0] + 1)**2
    loss_total = loss_DE + loss_initial
    
    return loss_total

```

