{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To Dos: 1. Data Generation 2. Plotte Density 3. Netzwerk definieren (probiere auch zusätzlich F in 0,1 zu constrainen) 4. Ableitung des Netzwerks bestimmen und plotten als Density Estimate und das mit dem echten vergleichen\n",
        "\n",
        "# Load Packages\n"
      ],
      "id": "ebdfc2bc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy import stats          # sampling\n",
        "\n",
        "import matplotlib.pyplot as plt  # visualization\n",
        "\n",
        "import torch                     # constructing, learning, using NNs\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "id": "6575ca61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Generation\n"
      ],
      "id": "c2bccff7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# construct 2-component Gaussian mixture distribution\n",
        "comp1 = stats.Normal( mu=2, sigma=0.25)\n",
        "comp2 = stats.Normal( mu=6, sigma=0.5)\n",
        "mix = stats.Mixture( [comp1, comp2], weights=[0.2, 0.8])\n",
        "\n",
        "#n = 100\n",
        "#x = mix.sample(n)"
      ],
      "id": "5b8332f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization of the analytic PDF\n"
      ],
      "id": "dc5cba49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "x = np.linspace(0, 10, 1000)\n",
        "plt.clf()\n",
        "plt.plot(x, mix.pdf(x))\n",
        "plt.title(\"Analytic PDF\")\n",
        "plt.show()"
      ],
      "id": "5ab98b51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import stats\n",
        "\n",
        "n = 100\n",
        "\n",
        "# construct 2-component Gaussian mixture distribution\n",
        "comp1 = stats.Normal(mu=-5, sigma=1)\n",
        "comp2 = stats.Normal(mu=5, sigma=1)\n",
        "mixture = stats.Mixture([comp1,comp2],weights=[0.4, 0.6])\n",
        "\n",
        "# sample from mixture distribution, sort and make torch tensor \n",
        "x = mixture.sample(n)\n",
        "x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)"
      ],
      "id": "01ebf67a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "u = np.random.uniform(0, 1, n)\n",
        "u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(Network, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(input_size, hidden_size),\n",
        "      nn.Sigmoid(),\n",
        "      nn.Linear(hidden_size, output_size)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomLoss, self).__init__()\n",
        "  \n",
        "  def forward(self, predictions, targets):\n",
        "    loss_squared = torch.mean((predictions - targets)**2)\n",
        "    return loss_squared\n",
        "\n",
        "\n",
        "class CustomLoss2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomLoss2, self).__init__()\n",
        "  \n",
        "  def forward(self, y_pred, y_true, lambda_mon, mon_d, mon_u):\n",
        "    loss_prediction = torch.mean((y_pred - y_true)**2)\n",
        "    \n",
        "    diff = mon_d - mon_u\n",
        "    penalty = torch.sigmoid(10000 * diff) * (diff**2)\n",
        "    loss_monotonicity = torch.mean(penalty)\n",
        "    \n",
        "    #loss_monotonicity = torch.mean(torch.heaviside(mon_d - mon_u, torch.tensor(0.0))*(mon_d - mon_u)**2)\n",
        "    loss_total = loss_prediction + lambda_mon * loss_monotonicity\n",
        "    return loss_total\n",
        "\n",
        "model = Network(input_size=1, hidden_size=30, output_size=1)\n",
        "#criterion = CustomLoss()\n",
        "criterion = CustomLoss2()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "lambda_mon = 1000000\n",
        "n_mon_points = 1000\n",
        "mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\n",
        "delta = 0.1*(max(x)-min(x))/n_mon_points\n",
        "\n",
        "num_epochs = 100\n",
        "steps_per_epoch = 200\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for step in range(steps_per_epoch):\n",
        "    preds = model(x)\n",
        "    u = np.random.uniform(0, 1, n)\n",
        "    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n",
        "    \n",
        "    mon_d = model(mon_points)\n",
        "    mon_u = model(mon_points + delta)\n",
        "    \n",
        "    #loss = criterion(preds, u)\n",
        "    \n",
        "    loss = criterion(preds, u, lambda_mon, mon_d, mon_u)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "xx = torch.linspace(-10, 10, 500).unsqueeze(1)\n",
        "with torch.no_grad():\n",
        "    yy = model(xx)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x.numpy(), u.numpy(), label=\"Target (sorted uniform)\", color='green')\n",
        "plt.plot(xx.numpy(), yy.numpy(), label=\"Learned CDF\", color='blue')\n",
        "plt.plot(xx.numpy(), mixture.cdf(xx), label=\"Learned CDF\", color='red')\n",
        "plt.title(\"NN fitting empirical CDF\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "a0ebb226",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 500\n",
        "\n",
        "comp1 = stats.norm(loc=-5, scale=1)  # Fix: stats.Normal → stats.norm with loc and scale\n",
        "comp2 = stats.norm(loc=5, scale=1)\n",
        "x = np.concatenate([\n",
        "    comp1.rvs(size=int(0.4 * n)),\n",
        "    comp2.rvs(size=int(0.6 * n))\n",
        "])\n",
        "np.random.shuffle(x)\n",
        "x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n",
        "\n",
        "u = np.random.uniform(0, 1, n)\n",
        "u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Network, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class CustomLoss2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss2, self).__init__()\n",
        "    \n",
        "    def forward(self, y_pred, y_true, lambda_mon, mon_d, mon_u):\n",
        "        loss_prediction = torch.mean((y_pred - y_true)**2)\n",
        "        diff = mon_d - mon_u\n",
        "        penalty = torch.clamp(diff, min=0.0)**2  # Use clamp to penalize downward slopes\n",
        "        loss_monotonicity = torch.mean(penalty)\n",
        "        loss_total = loss_prediction + lambda_mon * loss_monotonicity\n",
        "        return loss_total\n",
        "\n",
        "model = Network(input_size=1, hidden_size=30, output_size=1)\n",
        "criterion = CustomLoss2()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "lambda_mon = 1000000  # You can tune this\n",
        "n_mon_points = 1000\n",
        "mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\n",
        "delta = 0.1*(max(x)-min(x))/n_mon_points\n",
        "\n",
        "num_epochs = 100\n",
        "steps_per_epoch = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step in range(steps_per_epoch):\n",
        "        preds = model(x)\n",
        "        u = np.random.uniform(0, 1, n)\n",
        "        u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n",
        "        \n",
        "        mon_d = model(mon_points)\n",
        "        mon_u = model(mon_points + delta)\n",
        "        \n",
        "        loss = criterion(preds, u, lambda_mon, mon_d, mon_u)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "xx = torch.linspace(-10, 10, 500).unsqueeze(1)\n",
        "with torch.no_grad():\n",
        "    yy = model(xx)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x.numpy(), u.numpy(), label=\"Target (sorted uniform)\", color='green')\n",
        "plt.plot(xx.numpy(), yy.numpy(), label=\"Learned CDF\", color='blue')\n",
        "plt.plot(xx.numpy(), 0.4 * comp1.cdf(xx.numpy()) + 0.6 * comp2.cdf(xx.numpy()), label=\"True CDF (Mixture)\", color='red')\n",
        "plt.title(\"NN fitting empirical CDF\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "9e8acc15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#### DATA GENERATION ###\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "n = 100\n",
        "\n",
        "comp1 = stats.Normal(mu=-2, sigma=1)\n",
        "comp2 = stats.Normal(mu=2, sigma=1)\n",
        "mixture = stats.Mixture([comp1,comp2],weights=[0.4, 0.6])\n",
        "\n",
        "x = mixture.sample(n)"
      ],
      "id": "ca2b352e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "X1 = stats.Normal(mu=-2, sigma=1)\n",
        "X2 = stats.Normal(mu=2, sigma=1)\n",
        "mixture = stats.Mixture([X1,X2],weights=[0.4, 0.6])\n",
        "x = np.linspace(-10, 10, 300)\n",
        "plt.clf()\n",
        "plt.plot(x, mixture.pdf(x))\n",
        "plt.title('XXXX of normal distribution mixture')\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.hist(mixture.sample(1000), bins=20, density=True, alpha=0.7, color='blue')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# To Dos:\n",
        "# 1. define mixture\n",
        "# 2. sample from mixture"
      ],
      "id": "b674013b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}