---
title: "Neural Networks for Probability Density Estimation"
description: "Post description"
author: "Author"
date: "5/22/2021"
#image: "cover.jpg"
categories:
  - Deep Learning
  - Python
  - Probability Density Estimation
bibliography: references.bib
nocite: '@*'
---


# Introduction

The impressive capabilitis of neural networks in typical regression and classification tasks are widely known. Recently, I came across a paper about a lesser known but statistically very interesting application of neural networks that was written some years ago. The paper shows how neural networks can be used for estimating the probability density function of a random variable [@magdon1998neural]. Before implementing it from scratch in Python, I will first describe the general approach.

# General Approach

Let $X$ be a continuous random variable with unknown probability density function (PDF) $f_X$ and unknown cumulative density function (CDF) $F_X$. Given a set of $n \in \mathbb{N}$ i.i.d. realizations of $X$, $x_i, i=1, \dots, n$, the goal is to estimate $f_X$. In contrast to other approaches, such as Kernel Density Estimators, the approach by @magdon1998neural does not directly estimate the PDF.

Instead, the approach by @magdon1998neural relies on the following two useful properties of the **CDF**:

1.  If the CDF is differentiable, the PDF is obtained as the derivative of the CDF, i.e., $f_X(x)=\frac{d \, F_X}{d \, x}(x)$.
2.  Transforming a random variable by its own CDF yields a new random variable that is uniformly distributed on $(0,1)$, i.e., $F_X(X) \sim \mathcal{U}(0,1)$.

@magdon1998neural proposed to use a neural network, which needs to be differentiable, to approximate the **CDF** and take afterwards the first derivative w.r.t. $x$ to obtain an estimate of the **PDF**. Let $N_{\theta}(x)$ denote a feed forward multilayer perceptron parametrized by $\theta$ (i.e. a vector containing all weights and biases), $N_{\theta}: \mathbb{R} \to \mathbb{R}$. $N_{\theta}$ is differentiable, if the activation functions employed by it are differentiable (since it is then a composition of differentiable functions). $tanh$ and $sigmoid$ are examples of differentiable activation functions.

So the goal is to find the parameter vector $\theta^\star$ for which $N_{\theta^\star}(x) = F_X(x)$ for all $x \in \mathbb{R}$. However, that is the same as finding the parameter vector $\theta^\star$ for which the transformed random variable $N_{\theta^\star}(X)$ is uniformly distributed on $(0,1)$, since $F_X(X) \sim \mathcal{U}(0,1)$. This is achieved by the following procedure [@magdon1998neural]: Beforehand, the realizations of the random variable $X$ are sorted in ascending order, i.e. $x_{[1]} \leq \dots \leq x_{[n]}$. These $x_{[i]}, i=1,\dots, n$, serve as inputs. During training, in each iteration step <!--$t \in \mathbb{N}$-->, first, a new set of $n$ samples are drawn i.i.d. from a uniform distribution on $(0,1)$, $u_i \sim \mathcal{U}(0,1)$, and sorted in ascending order, i.e. $u_{[1]} \leq \dots \leq u_{[n]}$. These $u_{[i]}, i=1,\dots, n$, serve as targets only for that iteration step. Second, a gradient descent step is taken to adjust the parameter vector to better map the inputs onto the generated targets. For the purpose of mapping the data to a uniform distribution, the following loss function is a reasonable starting point: $$ L_{dist}(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ N_{\theta}(x_{[i]}) -  u_{[i]}\right]^2 $$

However, besides from mapping the data to a uniform distribution, there are two further properties, that CDFs exhibit and that the neural network approximation to the CDF must also have to be a legitimate estimate of a CDF. <!--Note that a CDF cannot be any arbitrary function. In fact, a CDF must have two further important properties that the neural network approximation to the CDF should also exhibit: --> First, the CDF maps $\mathbb{R}$ onto the interval $[0,1]$. Second, the CDF is monotonically increasing. By using an activation function in the output layer that maps $\mathbb{R}$ onto the interval $[0,1]$, it can be guaranteed that the neural network approximation to the CDF satisfies the first property. The second property can be enforced by modifying the loss function. The modified loss function $L_{total}$ also includes a term, $L_{mon}$, that applies a penalty if the neural network approximation to the CDF is not monotonically increasing in $x$. More specifically, $L_{mon}$ is written as: 
$$ L_{mon}(\theta) = \frac{1}{n_{mon}} \sum_{k=1}^{n_{mon}} \max \left( N_{\theta}(x_k^{mon}) - N_{\theta}(x_k^{mon} + \Delta), 0 \right)$$
for some small $\Delta > 0$ with $x_k^{mon}, k=1,\dots, n_{mon},$ being a set of $n_{mon} \in \mathbb{N}$ points at which monotonicity is to be enforced. 

The resulting modified loss function is: 
$$ L_{total}(\theta) = L_{dist}(\theta) + \lambda_{mon} \, L_{mon}(\theta) $$
where $\lambda_{mon} \in \mathbb{R}_{>0}$ is a large positive weighting factor for the monotonicity constraint. $\theta^\star$ is then given by:
$$\theta^\star = \underset{\theta}{\text{arg min}} \; L_{total}(\theta)$$
Once $\theta^\star$ is computed using standard gradient based methods, the estimated PDF $\hat{f}_X$ is obtained by
$$ \hat{f}_X(x) = \frac{\partial \, N_{\theta^\star}(x)}{\partial \, x}  $$

exploiting the differentiability of $N_{\theta}$ using backpropagation.


In summary, the procedure for estimating the **PDF** using neural network is given by:

1.  Sort the realizations of the random variable $X$ in ascending order, i.e., $x_{[1]} \leq \dots \leq x_{[n]}$.

2.  Initialize the parameter vector $\theta$ randomly.

3.  Repeat until convergence:

    1.  Draw $n$ i.i.d. realizations from a uniform distribution on $(0,1)$, $u_i \sim \mathcal{U}(0,1)$, and sort them in ascending order, i.e., $u_{[1]} \leq \dots \leq u_{[n]}$.

    2.  Update the parameter vector: $$ \theta \gets \theta - \eta \times \frac{\partial \, L_{total}(\theta)}{\partial \, \theta} $$

4.  Determine the estimated PDF as $$ \hat{f}_X(x) = \frac{\partial \, N_{\theta^\star}(x)}{\partial \, x}  $$ using backpropagation.



# Implementation

After detailing the general approach for solving ODEs using neural networks, in this section, I will implement it from scratch in Python and apply it to two different examples.

```{python}
# for sampling
import random
import numpy as np
from scipy import stats

# for visualization
import matplotlib.pyplot as plt

# for constructing, learning, using NNs
import torch
import torch.nn as nn
import torch.optim as optim
```

## Define Neural Network

At first, I define the neural network used for approximating the CDF. It is a fully connected feed forward neural network with one hidden layer using a $tanh$ activation function and an output layer using a $sigmoid$ activation function. Thereby, the neural network is differentiable and maps $\mathbb{R}$ onto the interval $(0,1)$.

```{python}
class N(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(N, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Tanh(),
      nn.Linear(hidden_size, output_size),
      nn.Sigmoid()
      )
    
  def forward(self, x):
    return self.net(x)

```

## Define Modified Loss Function

Next, I define the modified loss function as described above.

```{python}
class Loss(nn.Module):
  def __init__(self):
    super(Loss, self).__init__()
  
  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):
    loss_dist = torch.mean((y_pred - y_true)**2)
    
    diff = mon_l - mon_u
    loss_mon = torch.mean(torch.clamp(diff, min=0))
    
    loss_total = loss_dist + lambda_mon*loss_mon
    
    return loss_total

```

## Gaussian Mixture

For the first example, realizations will be drawn from a 2-component Gaussian mixture distribution. So I construct one with mixture weights $\phi_1=0.2$ and $\phi_2=0.8$, expectations $\mu_1=2$ and $\mu_2=5$ and standard deviations $\sigma_1=0.25$ and $\sigma_2=0.5$.

```{python}
# construct 2-component Gaussian mixture distribution
comp1 = stats.Normal(mu=2, sigma=0.25)
comp2 = stats.Normal(mu=5, sigma=0.5)
mix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])
```

The corresponding PDF looks as follows:

```{python}
#| code-fold: true
#| code-summary: "Show code"

x = np.linspace(0, 10, 1000)

plt.figure(figsize=(6,5))
plt.plot(x, mix.pdf(x), color="C0", linestyle="dotted")
plt.title("True Probability Density Function")
plt.xlabel("x")
plt.ylabel("Density")
plt.show()
```


### Set Seed

In the subsequent steps, the neural network will be initialized and realizations will be sampled from the PDF given above. Both involves randomness. Therefore, to ensure reproducibility of the subsequent steps, one must set a seed for the  random number generators.

```{python}
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
```

### Construct Model, Loss and Optimizer

After that, I instantiate the neural network, the loss function to be minimized and the optimizer (here, I use Adam).

```{python}
model = N(input_size=1, hidden_size=10, output_size=1)
criterion = Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

### Sample from PDF

Next, I sample $n=200$ i.i.d. realizations from the 2-component Gaussian mixture distribution. Subsequently, I sort then in ascending order.

```{python}
# sample data, sort it and convert to torch tensor
n = 200
rng = np.random.default_rng(seed)
x = mix.sample(n, rng=rng)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)
```


### Set Monotonicity Points

Before training can start, I have to specify $\lambda_{mon}$, $n_{mon}$, the monotonicity points themselves as well as $\Delta$. I choose to place $n_{mon}=1000$ monotonicity points equally spaced between the smallest and the largest sampled $X$-values.

```{python}
# set monotonicity points
lambda_mon = 1e6
n_mon = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon
```


### Training

Now, everything is set up and training can start. Observe that, as described in the general approach, in each iteration, a new set of $n$ samples are drawn i.i.d. from $\mathcal{U}(0,1)$ and sorted in ascending order. These serve as targets only for that iteration.

```{python}
#| output: false

num_epochs = 100
steps_per_epoch = 2500

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}", end="\r", flush=True)

```

### Plot CDF

Once training has finished, I plot the learned neural network approximation of CDF against the actual CDF of the 2-component Gaussian mixture.

```{python}
#| code-fold: true
#| code-summary: "Show code"

xx = torch.linspace(0, 10, 1000).unsqueeze(1)
with torch.no_grad():
    cdf_est = model(xx)

plt.figure(figsize=(6, 5))
plt.plot(xx.numpy(), cdf_est.numpy(), color="C3", label="Estimated CDF")
plt.plot(xx.numpy(), mix.cdf(xx), color="C0", label="True CDF", linestyle="dotted")
plt.title("Cumulative Density Function (CDF)")
plt.xlabel("x")
plt.ylabel("Cumulative Density")
plt.legend()
plt.show()

```

Obviously, the neural network approximation of the CDF given just a sample of 200 realizations of $X$ is very accurate.


### Compute PDF

Given the neural network approximation of the CDF, the following function computes an estimate of the PDF from it.

```{python}
def ComputePDF(model, x):
  x = x.requires_grad_(True)
  cdf = model(x)
  grad_outputs = torch.ones_like(cdf)
  pdf = torch.autograd.grad(
    outputs=cdf,
    inputs=x,
    grad_outputs=grad_outputs
    )[0]
  return pdf

```

### Plot PDF

This plot confirms, that the estimate of the PDF obtained from the neural network approximation to the CDF is quite precise.

```{python}
#| code-fold: true
#| code-summary: "Show code"

pdf_est = ComputePDF(model, xx).detach()
xx_np = xx.detach().numpy()


plt.figure(figsize=(6, 5))
plt.plot(xx_np, pdf_est.numpy(), color="C3", label="Estimated PDF")
plt.plot(xx_np, mix.pdf(xx_np), color="C0", label="True PDF", linestyle="dotted")
plt.title("Probability Density Function (PDF)")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()

```

## Weibull Distribution

As second example, I will use a Weibull distribution with $shape=1.8$ (and $scale=1$).

```{python}
# construct Weibull distribution
dist = stats.weibull_min(c=1.8)
```

A plot of its PDF shows that it is clearly skewed.

```{python}
#| code-fold: true
#| code-summary: "Show code"

x = np.linspace(0, 5, 1000)

plt.figure(figsize=(6,5))
plt.plot(x, dist.pdf(x), color="C0", linestyle="dotted")
plt.title("True Probability Density Function")
plt.xlabel("x")
plt.ylabel("Density")
plt.show()
```

The following steps (set seed; construct model, loss and optimizer; sample from PDF; set monontonicity points; training) are analogous to those in the first example. Therefore, I hide that part of the code. If you wish to have a look at it, click on the "Show code" button below.

```{python}
#| code-fold: true
#| code-summary: "Show code"
#| output: false

# set seed
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# construct model, loss and optimizer
model = N(input_size=1, hidden_size=10, output_size=1)
criterion = Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# sample data, sort it and convert to torch tensor
n = 200
rng = np.random.default_rng(seed)
x = dist.rvs(n, random_state=rng)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)

# set monotonicity points
lambda_mon = 1e6
n_mon = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon


num_epochs = 100
steps_per_epoch = 2500

# training
for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}", end="\r", flush=True)


```

### Plot CDF

Inspecting the below plot of the learned neural network approximation of the CDF and the actual CDF shows again that the neural network was successful in recovering the actual CDF.

```{python}
#| code-fold: true
#| code-summary: "Show code"

xx = torch.linspace(0, 5, 1000).unsqueeze(1)
with torch.no_grad():
    cdf_est = model(xx)

plt.figure(figsize=(6, 5))
plt.plot(xx.numpy(), cdf_est.numpy(), color="C3", label="Estimated CDF")
plt.plot(xx.numpy(), dist.cdf(xx), color="C0", label="True CDF", linestyle="dotted")
plt.title("Cumulative Density Function (CDF)")
plt.xlabel("x")
plt.ylabel("Cumulative Density")
plt.legend()
plt.show()

```

### Plot PDF

Also when comparing the resulting estimated PDF to the actual one, it seems that neural network approach yields good results.


```{python}
#| code-fold: true
#| code-summary: "Show code"

pdf_est = ComputePDF(model, xx).detach()
xx_np = xx.detach().numpy()


plt.figure(figsize=(6, 5))
plt.plot(xx_np, pdf_est.numpy(), color="C3", label="Estimated PDF")
plt.plot(xx_np, dist.pdf(xx_np), color="C0", label="True PDF", linestyle="dotted")
plt.title("Probability Density Function (PDF)")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()

```
