To Dos:
1. seed setzen für sampling UND neural network training
2. Plots:
  - visualization of histogram
  - vergleich cdfs
  - vergleich pdfs
4. habe jetzt als innere aktivierungsfunktion tanh, hatte davor sigmoid
5. Ableitung des Netzwerks bestimmen und plotten als Density Estimate und das mit dem echten vergleichen
6. INSIGHTS: Vermutung je weniger overlap zwischen den beiden Components desto; je größer die Sample Size desto genauer matcht NN der echten Verteilungsfunktion

# Load Packages

```{python}
import numpy as np
from scipy import stats           # for sampling

import matplotlib.pyplot as plt   # for visualization

import torch                      # for constructing, learning, using NNs
import torch.nn as nn
import torch.optim as optim
```

# Data Generation

```{python}
# construct 2-component Gaussian mixture distribution
comp1 = stats.Normal(mu=2, sigma=0.25)
comp2 = stats.Normal(mu=6, sigma=0.5)
mix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])

#n = 100
#x = mix.sample(n)
```

# Visualization of the analytic PDF

```{python}
#| echo: false

x = np.linspace(0, 10, 1000)
plt.clf()
plt.plot(x, mix.pdf(x))
plt.title("Analytic PDF")
plt.show()
```

# Visualization of Histogram

# Define Neural Network

```{python}
class Network(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(Network, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Tanh(),
      nn.Linear(hidden_size, output_size),
      nn.Sigmoid()
      )
    
  def forward(self, x):
    return self.net(x)

```

# Define Custom Loss Function

```{python}
class LossPDE(nn.Module):
  def __init__(self):
    super(LossPDE, self).__init__()
  
  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):
    loss_prediction = torch.mean((y_pred - y_true)**2)
    
    diff = mon_l - mon_u
    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))
    
    loss_total = loss_prediction + lambda_mon*loss_monotonicity
    
    return loss_total

```

# Construct model, loss and optimizer

```{python}
model = Network(input_size=1, hidden_size=10, output_size=1)
criterion = LossPDE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

# Sample data and set monotonicity points

```{python}
# sample data, sort it and convert to torch tensor
n = 200
x = mix.sample(n)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)

# set monotonicity points
lambda_mon = 1e6
n_mon_points = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon_points
```

# Training

```{python}
num_epochs = 200
steps_per_epoch = 1000

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

```



```{python}
import matplotlib.pyplot as plt
xx = torch.linspace(0, 10, 500).unsqueeze(1)
with torch.no_grad():
    yy = model(xx)

plt.figure(figsize=(10, 5))
#plt.plot(x.numpy(), u.numpy(), label="Target (sorted uniform)", color='green')
plt.plot(xx.numpy(), yy.numpy(), label="Learned CDF", color='blue')
plt.plot(xx.numpy(), mix.cdf(xx), label="Learned CDF", color='red')
plt.title("NN fitting empirical CDF")
plt.legend()
plt.show()

```


```{python}
# Define a function to compute derivative of the NN output wrt input
def compute_pdf_from_cdf(model, x):
    x = x.requires_grad_(True)  # enable grad w.r.t x
    cdf = model(x)
    grad_outputs = torch.ones_like(cdf)
    pdf = torch.autograd.grad(
        outputs=cdf,
        inputs=x,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    return pdf

# Create a grid of points where we want to evaluate PDF
xx = torch.linspace(0, 10, 100).unsqueeze(1)

# Compute CDF and PDF estimates
with torch.no_grad():
    cdf_est = model(xx)

pdf_est = compute_pdf_from_cdf(model, xx).detach()

# Compute true PDF from the mixture for comparison
xx_np = xx.detach().numpy().flatten()
true_pdf = mix.pdf(xx_np)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(xx_np, pdf_est.numpy(), label='NN PDF Estimate', color='blue')
plt.plot(xx_np, true_pdf, label='True PDF', color='red', linestyle='dashed')
plt.title("PDF estimated by derivative of NN CDF vs True PDF")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()
```