To Dos:
ACHTUNG BEI MEINER WEBSITE ERSCHEINT KOMISCHERWEISE IN URL ERGÄNZUNG T.S. SMITH; HABE ES EVTL GEÄNDERT MITTELS TITEL IN IDEX file in oberster Hierarchie
WICHTIG: Kommentar auch in ANN für ODEs bzgl. epochs & PRINT REPLACE ÜBERNEHMEN
3. Plots:
  - TRAININGSFORTSCHRITT
  - visualization of histogram
  - vergleich cdfs
  - vergleich pdfs
  - evtl. irgendein KDE für verschiedene Bandwidths
4. habe jetzt als innere aktivierungsfunktion tanh, hatte davor sigmoid
5. Ableitung des Netzwerks bestimmen und plotten als Density Estimate und das mit dem echten vergleichen

# Load Packages

```{python}
# for sampling
import numpy as np
from scipy import stats

# for visualization
import matplotlib.pyplot as plt

# for constructing, learning, using NNs
import torch
import torch.nn as nn
import torch.optim as optim
```

# Data Generation

```{python}
# construct 2-component Gaussian mixture distribution
comp1 = stats.Normal(mu=2, sigma=0.25)
comp2 = stats.Normal(mu=6, sigma=0.5)
mix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])
```

# Visualization of the analytic PDF

```{python}
#| echo: false

x = np.linspace(0, 10, 1000)

plt.figure(figsize=(10,5))
plt.plot(x, mix.pdf(x), colour="", linestyle="")
plt.title("True Probability Density Function")
plt.xlabel("Density")
plt.ylabel("x")
plt.show()
```

# Visualization of Histogram

# Define Neural Network

```{python}
class Network(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(Network, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Tanh(),
      nn.Linear(hidden_size, output_size),
      nn.Sigmoid()
      )
    
  def forward(self, x):
    return self.net(x)

```

# Define Custom Loss Function

```{python}
class LossPDE(nn.Module):
  def __init__(self):
    super(LossPDE, self).__init__()
  
  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):
    loss_prediction = torch.mean((y_pred - y_true)**2)
    
    diff = mon_l - mon_u
    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))
    
    loss_total = loss_prediction + lambda_mon*loss_monotonicity
    
    return loss_total

```

# Construct model, loss and optimizer

```{python}
model = Network(input_size=1, hidden_size=10, output_size=1)
criterion = LossPDE()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

# Sample data and set monotonicity points

```{python}
# sample data, sort it and convert to torch tensor
n = 200
rng = np.random.default_rng(42)
x = mix.sample(n, rng=rng)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)

# set monotonicity points
lambda_mon = 1e6
n_mon_points = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon_points
```

# Training

 wichtiger kommentar: in jedem step wird ganzer datensatz verwendet; epochs dienen nur dazu dass nicht in jedem step fortschritt gedruck wird

```{python}
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)

num_epochs = 200
steps_per_epoch = 1000

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
  
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}", end="\r", flush=True)

```



```{python}
xx = torch.linspace(0, 10, 1000).unsqueeze(1)
with torch.no_grad():
    yy = model(xx)

plt.figure(figsize=(10, 5))
plt.plot(xx.numpy(), yy.numpy(), label="Learned CDF")
plt.plot(xx.numpy(), mix.cdf(xx), label="True CDF", linestyle="dashed)
plt.title("Cumulative Density Function (CDF)")
plt.xlabel("x")
plt.ylabel("Cumulative Density")
plt.legend()
plt.show()

```


```{python}
# Define a function to compute derivative of the NN output wrt input
def compute_pdf_from_cdf(model, x):
    x = x.requires_grad_(True)  # enable grad w.r.t x
    cdf = model(x)
    grad_outputs = torch.ones_like(cdf)
    pdf = torch.autograd.grad(
        outputs=cdf,
        inputs=x,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]
    return pdf

# Create a grid of points where we want to evaluate PDF
xx = torch.linspace(0, 10, 1000).unsqueeze(1)

# Compute CDF and PDF estimates
with torch.no_grad():
    cdf_est = model(xx)

pdf_est = compute_pdf_from_cdf(model, xx).detach()

# Compute true PDF from the mixture for comparison
xx_np = xx.detach().numpy().flatten()
true_pdf = mix.pdf(xx_np)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(xx_np, pdf_est.numpy(), label="NN PDF Estimate")
plt.plot(xx_np, true_pdf, label="True PDF", linestyle='dashed')
plt.title("PDF estimated by derivative of NN CDF vs True PDF")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()
```