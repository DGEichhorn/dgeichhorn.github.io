---
title: "Neural Networks for Probability Density Estimation"
description: "Post description"
author: "Author"
date: "5/22/2021"
#image: "cover.jpg"
categories:
  - Deep Learning
  - Python
  - Probability Density Estimation
bibliography: references.bib
---

# TO DO: CODE AN BEZEICHNUNGEN IN FORMELN ANPASSEN

# SCHAUEN OB REIHENFOLGE DER CODE BLOCKS v.a. def NN und def loss eine anderes ergebnis liefern

# reihenfolge der code blocks verbessern

# Introduction

The impressive capabilities of neural networks in typical regression and classification tasks are widely known. Recently, I came across a paper about a lesser known but mathematically very interesting application of neural networks that was written some years ago. The paper shows how neural networks can be used for estimating the probability density function of a random variable [@magdon1998neural]. Before implementing it from scratch in Python, I will first describe the general approach.

# General Approach

Let $X$ be a continuous random variable with unknown probability density function (PDF) $f_X$ and unknown cumulative density function (CDF) $F_X$. Given a set of $n \in \mathbb{N}$ i.i.d. realizations of $X$, $x_i, i=1, \dots, n$, the goal is to estimate $f_X$. In contrast to other approaches, such as Kernel Density Estimators, the approach by @magdon1998neural does not directly estimate the PDF.

Instead, the approach by @magdon1998neural relies on the following two useful properties of the **CDF**:

1.  If the CDF is differentiable, the PDF is obtained as the derivative of the CDF, i.e., $f_X(x)=\frac{d \, F_X}{d \, x}(x)$.
2.  Transforming a random variable by its own CDF yields a new random variable that is uniformly distributed on \$(0,1)\$, i.e., $F_X(X) \sim \mathcal{U}(0,1)$.

@magdon1998neural proposed to use a neural network, which needs to be differentiable, to approximate the **CDF** and take afterwards the first derivative w.r.t. $x$ to obtain an estimate of the **PDF**. Let $N_{\theta}(x)$ denote a feed forward multilayer perceptron parametrized by $\theta$ (i.e. a vector containing all weights and biases), $N_{\theta}: \mathbb{R} \to \mathbb{R}$. $N_{\theta}$ is differentiable, if the activation functions employed by it are differentiable (since it is then a composition of differentiable functions). $tanh$ and $sigmoid$ are examples of differentiable activation functions.

So the goal is to find the parameter vector $\theta^\star$ for which $N_{\theta^\star}(x) = F_X(x)$ for all $x \in \mathbb{R}$. However, that is the same as finding the parameter vector $\theta^\star$ for which the transformed random variable $N_{\theta^\star}(X)$ is uniformly distributed on $(0,1)$, since $F_X(X) \sim \mathcal{U}(0,1)$. This is achieved by the following procedure [@magdon1998neural]: Beforehand, the realizations of the random variable $X$ are sorted in ascending order, i.e. $x_{[1]} \leq \dots \leq x_{[n]}$. These $x_{[i]}, i=1,\dots, n$, serve as inputs. During training, in each iteration step <!--$t \in \mathbb{N}$-->, first, a new set of $n$ samples are drawn i.i.d. from a uniform distribution on $(0,1)$, $u_i \sim \mathcal{U}(0,1)$, and sorted in ascending order, i.e. $u_{[1]} \leq \dots \leq u_{[n]}$. These $u_{[i]}, i=1,\dots, n$, serve as targets only for that iteration step. Second, a gradient descent step is taken to adjust the parameter vector to better map the inputs onto the generated targets. For the purpose of mapping the data to a uniform distribution, the following loss function is a reasonable starting point: $$ L_{Dist}(\theta) = \frac{1}{n} \sum_{i=1}^n \left[ N_{\theta}(x_{[i]}) -  u_{[i]}\right]^2 $$

However, besides from mapping the data to a uniform distribution, there are two further properties, that CDFs exhibit and that the neural network approximation to the CDF must also have to be a legitimate estimate of a CDF. <!--Note that a CDF cannot be any arbitrary function. In fact, a CDF must have two further important properties that the neural network approximation to the CDF should also exhibit: --> First, the CDF maps $\mathbb{R}$ onto the interval $[0,1]$. Second, the CDF is monotonically increasing. By using an activation function in the output layer that maps $\mathbb{R}$ onto the interval $[0,1]$, it can be guaranteed that the neural network approximation to the CDF satisfies the first property. The second property can be enforced by modifying the loss function. The modified loss function $L_{total}$ also includes a term, $L_{Mon}$, that applies a penalty if the neural network approximation to the CDF is not monotonically increasing in $x$. More specifically, $L_{Mon}$ is written as: $$ L_{Mon}(\theta) = \frac{1}{n_{Mon}} \sum_{k=1}^{n_{Mon}} \max \left( N_{\theta}(x_k^{Mon}) - N_{\theta}(x_k^{Mon} + \Delta), 0 \right)$$ for some small $\Delta > 0$ with $x_k^{Mon}, k=1,\dots, n_{Mon},$ being a set of $n_{Mon} \in \mathbb{N}$ points at which monotonicity is to be enforced. 

The resulting modified loss function is: $$ L_{total}(\theta) = L_{Dist}(\theta) + \lambda \, L_{Mon}(\theta) $$ where $\lambda \in \mathbb{R}_{>0}$ is a large positive weighting factor for the monotonicity constraint. $\theta^\star$ is then given by:
$$\theta^\star = \underset{\theta}{\text{arg min}} \; L_{total}(\theta)$$
Once $\theta^\star$ is computed using standard gradient based methods, the estimated PDF $\hat{f}_X$ is obtained by
$$ \hat{f}_X(x) = \frac{\partial \, N_{\theta^\star}(x)}{\partial \, x}  $$

exploiting the differentiability of $N_{\theta}$ using backpropagation.


In summary, the procedure for estimating the **PDF** using neural network is given by:

1.  Sort the realizations of the random variable $X$ in ascending order, i.e., $x_{[1]} \leq \dots \leq x_{[n]}$.

2.  Initialize the parameter vector $\theta$ randomly.

3.  Repeat until convergence:

    1.  Draw $n$ i.i.d. realizations from a uniform distribution on $(0,1)$, $u_i \sim \mathcal{U}(0,1)$, and sort them in ascending order, i.e., $u_{[1]} \leq \dots \leq u_{[n]}$.

    2.  Update the parameter vector: $$ \theta \gets \theta - \eta \times \frac{\partial \, L_{total}(\theta)}{\partial \, \theta} $$

4.  Determine the estimated PDF as $$ \hat{f}_X(x) = \frac{\partial \, N_{\theta^\star}(x)}{\partial \, x}  $$ using backpropagation.



# Implementation

After detailing the general approach for solving ODEs using neural networks, in this section, I will implement it from scratch in Python and apply it to two different examples.

```{python}
# for sampling
import random
import numpy as np
from scipy import stats

# for visualization
import matplotlib.pyplot as plt

# for constructing, learning, using NNs
import torch
import torch.nn as nn
import torch.optim as optim
```

## Define Neural Network

At first, I define the neural network used for approximating the CDF. It is a fully connected feed forward neural network with one hidden layer using a $tanh$ activation function and an output layer using a $sigmoid$ activation function. Thereby, the neural network is differentiable and maps $\mathbb{R}$ onto the interval $(0,1)$.

```{python}
class N(nn.Module):
  def __init__(self, input_size, hidden_size, output_size):
    super(N, self).__init__()
    self.net = nn.Sequential(
      nn.Linear(input_size, hidden_size),
      nn.Tanh(),
      nn.Linear(hidden_size, output_size),
      nn.Sigmoid()
      )
    
  def forward(self, x):
    return self.net(x)

```

## Define Modified Loss Function

```{python}
class Loss(nn.Module):
  def __init__(self):
    super(Loss, self).__init__()
  
  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):
    loss_prediction = torch.mean((y_pred - y_true)**2)
    
    diff = mon_l - mon_u
    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))
    
    loss_total = loss_prediction + lambda_mon*loss_monotonicity
    
    return loss_total

```

## Set Seed for reproducibility of data generation and neural network training

```{python}
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
```

## Data Generation

```{python}
# construct 2-component Gaussian mixture distribution
comp1 = stats.Normal(mu=2, sigma=0.25)
comp2 = stats.Normal(mu=5, sigma=0.5)
mix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])
```

## Visualization of the analytic PDF

```{python}
#| code-fold: true
#| code-summary: "Show code"

x = np.linspace(0, 10, 1000)

plt.figure(figsize=(6,5))
plt.plot(x, mix.pdf(x), color="C0", linestyle="dotted")
plt.title("True Probability Density Function")
plt.xlabel("Density")
plt.ylabel("x")
plt.show()
```


## Construct model, loss and optimizer

```{python}
model = N(input_size=1, hidden_size=10, output_size=1)
criterion = Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
```

## Sample data and set monotonicity points

```{python}
# sample data, sort it and convert to torch tensor
n = 200
rng = np.random.default_rng(seed)
x = mix.sample(n, rng=rng)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)

# set monotonicity points
lambda_mon = 1e6
n_mon_points = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon_points
```

## Training

wichtiger kommentar: in jedem step wird ganzer datensatz verwendet; epochs dienen nur dazu dass nicht in jedem step fortschritt gedruck wird

```{python}
#| output: false

num_epochs = 100
steps_per_epoch = 2500

for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}", end="\r", flush=True)

```

## plot cdf

```{python}
#| code-fold: true
#| code-summary: "Show code"

xx = torch.linspace(0, 10, 1000).unsqueeze(1)
with torch.no_grad():
    cdf_est = model(xx)

plt.figure(figsize=(6, 5))
plt.plot(xx.numpy(), cdf_est.numpy(), color="C3", label="Estimated CDF")
plt.plot(xx.numpy(), mix.cdf(xx), color="C0", label="True CDF", linestyle="dotted")
plt.title("Cumulative Density Function (CDF)")
plt.xlabel("x")
plt.ylabel("Cumulative Density")
plt.legend()
plt.show()

```

## Function to compute derivative of NN w.r.t. to input x

```{python}
def ComputePDF(model, x):
  x = x.requires_grad_(True)
  cdf = model(x)
  grad_outputs = torch.ones_like(cdf)
  pdf = torch.autograd.grad(
    outputs=cdf,
    inputs=x,
    grad_outputs=grad_outputs
    )[0]
  return pdf

```

## Plot PDF

```{python}
#| code-fold: true
#| code-summary: "Show code"

pdf_est = ComputePDF(model, xx).detach()
xx_np = xx.detach().numpy()


plt.figure(figsize=(6, 5))
plt.plot(xx_np, pdf_est.numpy(), color="C3", label="Estimated PDF")
plt.plot(xx_np, mix.pdf(xx_np), color="C0", label="True PDF", linestyle="dotted")
plt.title("Probability Density Function (PDF)")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()

```

## Data Generation

```{python}
# construct Weibull distribution
dist = stats.weibull_min(c=1.8)
```

## Visualization of the analytic PDF

```{python}
#| code-fold: true
#| code-summary: "Show code"

x = np.linspace(0, 5, 1000)

plt.figure(figsize=(6,5))
plt.plot(x, dist.pdf(x), color="C0", linestyle="dotted")
plt.title("True Probability Density Function")
plt.xlabel("Density")
plt.ylabel("x")
plt.show()
```

## Set Seed for reproducibility of data generation and neural network training

```{python}
#| code-fold: true
#| code-summary: "Show code"
#| output: false

# set seed
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

# construct model, loss and optimizer
model = N(input_size=1, hidden_size=10, output_size=1)
criterion = Loss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# sample data, sort it and convert to torch tensor
n = 200
rng = np.random.default_rng(seed)
x = dist.rvs(n, random_state=rng)
x = torch.from_numpy(np.sort(x)).float().unsqueeze(1)

# set monotonicity points
lambda_mon = 1e6
n_mon_points = 1000
mon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]
delta = 0.1*(max(x)-min(x))/n_mon_points


num_epochs = 100
steps_per_epoch = 2500

# training
for epoch in range(num_epochs):
  for step in range(steps_per_epoch):
    preds = model(x)
    
    u = np.random.uniform(0, 1, n)
    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)
    
    mon_l = model(mon_points)
    mon_u = model(mon_points + delta)
    
    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}", end="\r", flush=True)


```

## plot cdf

```{python}
#| code-fold: true
#| code-summary: "Show code"

xx = torch.linspace(0, 5, 1000).unsqueeze(1)
with torch.no_grad():
    cdf_est = model(xx)

plt.figure(figsize=(6, 5))
plt.plot(xx.numpy(), cdf_est.numpy(), color="C3", label="Estimated CDF")
plt.plot(xx.numpy(), dist.cdf(xx), color="C0", label="True CDF", linestyle="dotted")
plt.title("Cumulative Density Function (CDF)")
plt.xlabel("x")
plt.ylabel("Cumulative Density")
plt.legend()
plt.show()

```

## Plot PDF

```{python}
#| code-fold: true
#| code-summary: "Show code"

pdf_est = ComputePDF(model, xx).detach()
xx_np = xx.detach().numpy()


plt.figure(figsize=(6, 5))
plt.plot(xx_np, pdf_est.numpy(), color="C3", label="Estimated PDF")
plt.plot(xx_np, dist.pdf(xx_np), color="C0", label="True PDF", linestyle="dotted")
plt.title("Probability Density Function (PDF)")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.show()

```
