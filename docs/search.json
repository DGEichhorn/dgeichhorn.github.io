[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My journey has taken me from studying maths and business at TUM to diving deep into statistics and machine learning. Currently, I am a Ph.D. candidate at the Chair of Statistics at the University of the Bundeswehr Munich. I have already had the opportunity to gather some practical experience through internships (e.g. at Allianz) and project work (PwC in cooperation with TUM Data Innovation Lab).\nFor me, work is most fun when you develop together data-driven solutions and put them into practice. I love expanding my skills continuously, e.g., by taking online courses or reading scientific literature on topics that are new to me.\nIn my leisure time, I enjoy rowing in a crew, cooking, travelling and reading."
  },
  {
    "objectID": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "href": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "",
    "text": "Load Packages\n\n# for sampling\nimport random\nimport numpy as np\nfrom scipy import stats\n\n# for visualization\nimport matplotlib.pyplot as plt\n\n# for constructing, learning, using NNs\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\nSet Seed for reproducibility of data generation and neural network training\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x221ffd567d0>\n\n\n\n\nData Generation\n\n# construct 2-component Gaussian mixture distribution\ncomp1 = stats.Normal(mu=2, sigma=0.25)\ncomp2 = stats.Normal(mu=5, sigma=0.5)\nmix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])\n\n\n\nVisualization of the analytic PDF\n\n\nShow code\nx = np.linspace(0, 10, 1000)\n\nplt.figure(figsize=(6,5))\nplt.plot(x, mix.pdf(x), color=\"C0\", linestyle=\"dotted\")\nplt.title(\"True Probability Density Function\")\nplt.xlabel(\"Density\")\nplt.ylabel(\"x\")\nplt.show()\n\n\n\n\n\n\n\nDefine Neural Network\n\nclass Network(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(Network, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Tanh(),\n      nn.Linear(hidden_size, output_size),\n      nn.Sigmoid()\n      )\n    \n  def forward(self, x):\n    return self.net(x)\n\n\n\nDefine Custom Loss Function\n\nclass LossPDE(nn.Module):\n  def __init__(self):\n    super(LossPDE, self).__init__()\n  \n  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):\n    loss_prediction = torch.mean((y_pred - y_true)**2)\n    \n    diff = mon_l - mon_u\n    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))\n    \n    loss_total = loss_prediction + lambda_mon*loss_monotonicity\n    \n    return loss_total\n\n\n\nConstruct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossPDE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSample data and set monotonicity points\n\n# sample data, sort it and convert to torch tensor\nn = 200\nrng = np.random.default_rng(seed)\nx = mix.sample(n, rng=rng)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon_points = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon_points\n\n\n\nTraining\nwichtiger kommentar: in jedem step wird ganzer datensatz verwendet; epochs dienen nur dazu dass nicht in jedem step fortschritt gedruck wird\n\nnum_epochs = 100\nsteps_per_epoch = 2500\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\", end=\"\\r\", flush=True)\n\n\n\nplot cdf\n\n\nShow code\nxx = torch.linspace(0, 10, 1000).unsqueeze(1)\nwith torch.no_grad():\n    cdf_est = model(xx)\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx.numpy(), cdf_est.numpy(), color=\"C3\", label=\"Estimated CDF\")\nplt.plot(xx.numpy(), mix.cdf(xx), color=\"C0\", label=\"True CDF\", linestyle=\"dotted\")\nplt.title(\"Cumulative Density Function (CDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Cumulative Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFunction to compute derivative of NN w.r.t. to input x\n\ndef ComputePDF(model, x):\n  x = x.requires_grad_(True)\n  cdf = model(x)\n  grad_outputs = torch.ones_like(cdf)\n  pdf = torch.autograd.grad(\n    outputs=cdf,\n    inputs=x,\n    grad_outputs=grad_outputs\n    )[0]\n  return pdf\n\n\n\nPlot PDF\n\n\nShow code\npdf_est = ComputePDF(model, xx).detach()\nxx_np = xx.detach().numpy()\n\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx_np, pdf_est.numpy(), color=\"C3\", label=\"Estimated PDF\")\nplt.plot(xx_np, mix.pdf(xx_np), color=\"C0\", label=\"True PDF\", linestyle=\"dotted\")\nplt.title(\"Probability Density Function (PDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nData Generation\n\n# construct Weibull distribution\ndist = stats.weibull_min(c=1.8)\n\n\n\nVisualization of the analytic PDF\n\n\nShow code\nx = np.linspace(0, 5, 1000)\n\nplt.figure(figsize=(6,5))\nplt.plot(x, dist.pdf(x), color=\"C0\", linestyle=\"dotted\")\nplt.title(\"True Probability Density Function\")\nplt.xlabel(\"Density\")\nplt.ylabel(\"x\")\nplt.show()\n\n\n\n\n\n\n\nSet Seed for reproducibility of data generation and neural network training\n\n\nShow code\n# set seed\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# construct model, loss and optimizer\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossPDE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# sample data, sort it and convert to torch tensor\nn = 200\nrng = np.random.default_rng(seed)\nx = dist.rvs(n, random_state=rng)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon_points = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon_points\n\n\nnum_epochs = 100\nsteps_per_epoch = 2500\n\n# training\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\", end=\"\\r\", flush=True)\n\n\n\n\nplot cdf\n\n\nShow code\nxx = torch.linspace(0, 5, 1000).unsqueeze(1)\nwith torch.no_grad():\n    cdf_est = model(xx)\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx.numpy(), cdf_est.numpy(), color=\"C3\", label=\"Estimated CDF\")\nplt.plot(xx.numpy(), dist.cdf(xx), color=\"C0\", label=\"True CDF\", linestyle=\"dotted\")\nplt.title(\"Cumulative Density Function (CDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Cumulative Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nPlot PDF\n\n\nShow code\npdf_est = ComputePDF(model, xx).detach()\nxx_np = xx.detach().numpy()\n\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx_np, pdf_est.numpy(), color=\"C3\", label=\"Estimated PDF\")\nplt.plot(xx_np, dist.pdf(xx_np), color=\"C0\", label=\"True PDF\", linestyle=\"dotted\")\nplt.title(\"Probability Density Function (PDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "href": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "title": "Dominik G. Eichhorn",
    "section": "",
    "text": "Daten können unter diesem Link downgeloaded werden: “https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip”\n\ngetwd()\n\n[1] \"C:/Users/domin/Documents/PersonalWebsite/dgeichhorn.github.io/blog/CopulasForFF5Factors\"\n\nlibrary(readr)\ndt <- read_csv(\"F-F_Research_Data_5_Factors_2x3.csv\",\n               skip = 4, n_max = 743,\n               col_names = c(\"Date\", \"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"))\n\nRows: 743 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): Date, Mkt_RF, SMB, HML, RMW, CMA, RF\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(dt)\n\ncor_matrix <- cor(dt[, c(\"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\")])\nprint(cor_matrix)\n\n           Mkt_RF          SMB          HML          RMW          CMA\nMkt_RF  1.0000000  0.277162682 -0.205902792 -0.186881612 -0.353833434\nSMB     0.2771627  1.000000000  0.006444189 -0.348613074 -0.087387014\nHML    -0.2059028  0.006444189  1.000000000  0.085160500  0.682067617\nRMW    -0.1868816 -0.348613074  0.085160500  1.000000000 -0.003665752\nCMA    -0.3538334 -0.087387014  0.682067617 -0.003665752  1.000000000\n\nplot(dt$HML, dt$CMA)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Post description for first post\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this post, I implement from scratch two different approaches for learning a ranking from data on the results of pairwise comparisons.\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPost description\n\n\n\n\n\n\nMay 22, 2021\n\n\nAuthor\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPost description for second post\n\n\n\n\n\n\nMay 23, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing from scratch a neural network based approach for approximating numerically the solutions of ordinary differential equations.\n\n\n\n\n\n\nMay 22, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/LearningToRank/LearningToRank.html",
    "href": "blog/LearningToRank/LearningToRank.html",
    "title": "Learning to Rank",
    "section": "",
    "text": "Get pairwise comparison data\nget American Football data from https://www.pro-football-reference.com/years/2024/games.htm; OLD NHL: https://www.hockey-reference.com/leagues/NHL_2025_games.html#games\n\nlibrary(rvest)        # for web page scraping\nlibrary(dplyr)        # for data manipulations\nlibrary(ggplot2)      # for visualization\n\nurl <- \"https://www.pro-football-reference.com/years/2024/games.htm#games\"\npage <- read_html(url)\n\ngames <- page %>%\n  html_node(\"table#games\") %>%\n  html_table(header=TRUE) %>%\n  as.data.frame()\n\ngames <- games[, c(1,5,7)]\nnames(games) <- c(\"Week\", \"Winner\", \"Loser\")\n\n# keep only games during regular season (i.e. int-valued week)\ngames <- games %>%\n  filter(grepl(\"^[0-9]+$\", Week)) %>%\n  select(Winner, Loser)\n\nteams <- sort(unique(c(games$Winner, games$Loser)))\nn_teams <- length(teams)\n\n\n\nConstruct Win Matrix from pairwise comparisons data\nadjacency matrix \\(W=[w_{ij}]_{i=1,\\dots,n,\\, j=1,\\dots, n}\\) i.e. i-th row says how often team i won against every other team\n\n# initialize win matrix with zeros\nW <- matrix(\n  rep(0, n_teams^2),\n  nrow = n_teams,\n  dimnames = list(\n    teams, # row names\n    teams  # col names\n  )\n)\n\n# fill win matrix\n# iterate over all games\nfor (g in 1:dim(games)[1]) {\n  W[games[g, \"Winner\"], games[g, \"Loser\"]] <- \n    W[games[g, \"Winner\"], games[g, \"Loser\"]] + 1\n}\n\n\n\nDefine function for mle estimation of Bradley Terry Model\n\nbradleyterry_mle <- function(W, n_iter=1000){\n  p <- dim(W)[1]\n  \n  theta.old <- rep(1, p)\n  names(theta.old) <- colnames(W)\n  \n  theta.new <- numeric(p)\n  names(theta.new) <- colnames(W)\n  \n  for (k in 1:n_iter) {\n    \n    for (i in 1:p) {\n      theta.new[i] <- \n        sum(W[i,])/(sum((W[i,] + W[,i])/(theta.old[i]+theta.old)))\n    }\n    \n    # divide by geomtric mean\n    theta.new <- theta.new/prod(theta.new)^(1/p)\n    \n    theta.old <- theta.new\n  }\n  \n  return(theta.new)\n}\n\napply mle estimation function to W matrix\n\nbt_mle <- bradleyterry_mle(W)\n\n\n\nPlot ranking results\n\ndf <- data.frame(\n  team = names(bt_mle),\n  score = as.numeric(bt_mle)\n)\n\nggplot(df, aes(reorder(team, score), y=score)) +\n  geom_col(fill=\"steelblue\") +\n  coord_flip() +\n  labs(title=\"TITEL\",\n       y=\"y ACHSE\",\n       x=\"x ACHSE\") +\n  theme_minimal()\n\n\n\n\n\n\ncompare my results to results of R Package Bradley Terry\n\n# evtl nicht die ganze library laden sondern nur BradleyTerry2:: an den notwendigen Stellen nutzen\nlibrary(BradleyTerry2)\n\nb<-countsToBinomial(W)\npackage_model <- BTm(cbind(win1, win2), player1, player2, data=b)\nBTabilities(package_model)\n\n                         ability      s.e.\nArizona Cardinals      0.0000000 0.0000000\nAtlanta Falcons       -0.1045449 0.8415298\nBaltimore Ravens       1.1418104 0.8676834\nBuffalo Bills          1.1877988 0.8601418\nCarolina Panthers     -1.1014454 0.8393499\nChicago Bears         -0.9715400 0.8249687\nCincinnati Bengals    -0.0567534 0.8709882\nCleveland Browns      -1.8616368 0.9452417\nDallas Cowboys        -0.4066470 0.8489731\nDenver Broncos         0.3610488 0.8457617\nDetroit Lions          2.5050022 0.9885558\nGreen Bay Packers      0.9721904 0.8207348\nHouston Texans         0.1539970 0.8664170\nIndianapolis Colts    -0.6713506 0.8470376\nJacksonville Jaguars  -1.9802343 0.8936397\nKansas City Chiefs     2.1471437 1.0010453\nLas Vegas Raiders     -1.3872235 0.8926616\nLos Angeles Chargers   0.5523284 0.8457519\nLos Angeles Rams       0.4710335 0.7428602\nMiami Dolphins        -0.7946729 0.7850284\nMinnesota Vikings      1.9035464 0.9331217\nNew England Patriots  -1.7985685 0.8431329\nNew Orleans Saints    -1.0977606 0.8733501\nNew York Giants       -1.6615983 0.9370010\nNew York Jets         -1.3766081 0.8251299\nPhiladelphia Eagles    1.5252550 0.9178396\nPittsburgh Steelers    0.3596589 0.8594350\nSan Francisco 49ers   -0.4485519 0.7422259\nSeattle Seahawks       0.4412204 0.7354132\nTampa Bay Buccaneers   0.4422769 0.8533135\nTennessee Titans      -2.1339592 0.9368089\nWashington Commanders  0.7176168 0.8500985\n\n# Converting my values into BTm ability values\nlog(bt_mle/bt_mle[1])\n\n    Arizona Cardinals       Atlanta Falcons      Baltimore Ravens \n            0.0000000            -0.1045449             1.1418104 \n        Buffalo Bills     Carolina Panthers         Chicago Bears \n            1.1877988            -1.1014454            -0.9715400 \n   Cincinnati Bengals      Cleveland Browns        Dallas Cowboys \n           -0.0567534            -1.8616368            -0.4066470 \n       Denver Broncos         Detroit Lions     Green Bay Packers \n            0.3610488             2.5050022             0.9721904 \n       Houston Texans    Indianapolis Colts  Jacksonville Jaguars \n            0.1539970            -0.6713506            -1.9802343 \n   Kansas City Chiefs     Las Vegas Raiders  Los Angeles Chargers \n            2.1471437            -1.3872235             0.5523284 \n     Los Angeles Rams        Miami Dolphins     Minnesota Vikings \n            0.4710335            -0.7946729             1.9035464 \n New England Patriots    New Orleans Saints       New York Giants \n           -1.7985685            -1.0977606            -1.6615983 \n        New York Jets   Philadelphia Eagles   Pittsburgh Steelers \n           -1.3766081             1.5252550             0.3596589 \n  San Francisco 49ers      Seattle Seahawks  Tampa Bay Buccaneers \n           -0.4485519             0.4412204             0.4422769 \n     Tennessee Titans Washington Commanders \n           -2.1339592             0.7176168 \n\n# Converting BTm values to my values\nBTabilities <- BTabilities(package_model)[,1]\nexp(BTabilities)*bt_mle[1]\n\n    Arizona Cardinals       Atlanta Falcons      Baltimore Ravens \n            1.0972960             0.9883723             3.4372074 \n        Buffalo Bills     Carolina Panthers         Chicago Bears \n            3.5989702             0.3647306             0.4153262 \n   Cincinnati Bengals      Cleveland Browns        Dallas Cowboys \n            1.0367550             0.1705396             0.7306666 \n       Denver Broncos         Detroit Lions     Green Bay Packers \n            1.5744371            13.4348377             2.9009532 \n       Houston Texans    Indianapolis Colts  Jacksonville Jaguars \n            1.2799820             0.5607379             0.1514673 \n   Kansas City Chiefs     Las Vegas Raiders  Los Angeles Chargers \n            9.3932631             0.2740692             1.9063251 \n     Los Angeles Rams        Miami Dolphins     Minnesota Vikings \n            1.7574827             0.4956804             7.3624697 \n New England Patriots    New Orleans Saints       New York Giants \n            0.1816417             0.3660770             0.2083055 \n        New York Jets   Philadelphia Eagles   Pittsburgh Steelers \n            0.2769941             5.0435188             1.5722503 \n  San Francisco 49ers      Seattle Seahawks  Tampa Bay Buccaneers \n            0.7006807             1.7058601             1.7076632 \n     Tennessee Titans Washington Commanders \n            0.1298844             2.2489553 \n\n\n\n\nGoogle Page Rank from scratch\n\np <- n_teams\n\nd <- 0.85\n\nr <- rep(1/p, p)\n\n# W_ij: number of times team j lost against team i\n#W\n\n# c_j: number of times team j lost (=colSums of W)\n\nc <- colSums(W)\n\nA <- (1-d)*rep(1,p) %*% t(rep(1,p))/p + d*W%*%diag(1/c) \n\nfor (i in 1:100) {\n  r <- A%*%r\n}\n\nprint(r)\n\n                             [,1]\nArizona Cardinals     0.023078736\nAtlanta Falcons       0.038286369\nBaltimore Ravens      0.061258396\nBuffalo Bills         0.071380275\nCarolina Panthers     0.013294979\nChicago Bears         0.016268901\nCincinnati Bengals    0.023895114\nCleveland Browns      0.020751663\nDallas Cowboys        0.026341877\nDenver Broncos        0.048459390\nDetroit Lions         0.056625881\nGreen Bay Packers     0.028293675\nHouston Texans        0.031835544\nIndianapolis Colts    0.017257968\nJacksonville Jaguars  0.009367408\nKansas City Chiefs    0.058884281\nLas Vegas Raiders     0.017925304\nLos Angeles Chargers  0.030433727\nLos Angeles Rams      0.043601426\nMiami Dolphins        0.018825457\nMinnesota Vikings     0.034682922\nNew England Patriots  0.024486948\nNew Orleans Saints    0.013432521\nNew York Giants       0.011337971\nNew York Jets         0.013269218\nPhiladelphia Eagles   0.047576348\nPittsburgh Steelers   0.041487365\nSan Francisco 49ers   0.021443657\nSeattle Seahawks      0.030969888\nTampa Bay Buccaneers  0.058166828\nTennessee Titans      0.011932274\nWashington Commanders 0.035147688\n\n\n\n\ncompare my results to R Package results\nwichtig: evtl. igraph:: verwenden anstatt gesamte library zu laden um klarer zu machen wo genau wir eigentlich was aus dieser library verwende\n\nlibrary(igraph)\n\nWarning: Paket 'igraph' wurde unter R Version 4.2.3 erstellt\n\n\n\nAttache Paket: 'igraph'\n\n\nDie folgenden Objekte sind maskiert von 'package:dplyr':\n\n    as_data_frame, groups, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    decompose, spectrum\n\n\nDas folgende Objekt ist maskiert 'package:base':\n\n    union\n\ngraphObj <- graph_from_adjacency_matrix(t(W), weighted = TRUE, mode = \"directed\")\n(prVec <- page_rank(graphObj)$vector)\n\n    Arizona Cardinals       Atlanta Falcons      Baltimore Ravens \n          0.023078736           0.038286369           0.061258396 \n        Buffalo Bills     Carolina Panthers         Chicago Bears \n          0.071380275           0.013294979           0.016268901 \n   Cincinnati Bengals      Cleveland Browns        Dallas Cowboys \n          0.023895114           0.020751663           0.026341877 \n       Denver Broncos         Detroit Lions     Green Bay Packers \n          0.048459390           0.056625881           0.028293675 \n       Houston Texans    Indianapolis Colts  Jacksonville Jaguars \n          0.031835544           0.017257968           0.009367408 \n   Kansas City Chiefs     Las Vegas Raiders  Los Angeles Chargers \n          0.058884281           0.017925304           0.030433727 \n     Los Angeles Rams        Miami Dolphins     Minnesota Vikings \n          0.043601426           0.018825457           0.034682922 \n New England Patriots    New Orleans Saints       New York Giants \n          0.024486948           0.013432521           0.011337971 \n        New York Jets   Philadelphia Eagles   Pittsburgh Steelers \n          0.013269218           0.047576348           0.041487365 \n  San Francisco 49ers      Seattle Seahawks  Tampa Bay Buccaneers \n          0.021443657           0.030969888           0.058166828 \n     Tennessee Titans Washington Commanders \n          0.011932274           0.035147688"
  },
  {
    "objectID": "blog/LibrariesInUK/LibrariesInUK.html",
    "href": "blog/LibrariesInUK/LibrariesInUK.html",
    "title": "Dominik G. Eichhorn",
    "section": "",
    "text": "Link für Plotting/Creating Maps in R: “https://datatricks.co.uk/creating-maps-in-r-2019”\nDaten: - Nutzung von Libraries: https://www.ons.gov.uk/explore-local-statistics/indicators/visited-a-public-library - Erreichbarkeit von Libraries: https://www.ons.gov.uk/explore-local-statistics/indicators/library-walk\nTo create a UK map heatmap at the Lower Tier Local Authority (LTLA / Unitary Authority) level with ggplot2, you’ll need:\nA shapefile (or geojson) of UK local authorities (LTLA/UA level).\nA dataset with values you want to plot (e.g., rates, counts, etc.), keyed by local authority code (e.g., lad22cd from ONS).\nJoining the shapefile with your data and plotting via geom_sf().\nHere’s a reproducible example in R:\n\n# Install if needed\n# install.packages(c(\"sf\", \"ggplot2\", \"dplyr\"))\n\n# library(sf)\n# library(ggplot2)\n# library(dplyr)\n# \n# # --- Step 1: Load UK Local Authority boundaries (ONS Geoportal provides shapefiles) ---\n# # Example: use Local Authority Districts (2022) boundaries (generalised, clipped)\n# # Download from: https://geoportal.statistics.gov.uk/\n# shapefile <- \"Local_Authority_Districts_(December_2022)_UK_BUC.shp\"\n# uk_lad <- st_read(shapefile)\n# \n# # Inspect codes\n# head(uk_lad)\n# \n# # --- Step 2: Example data ---\n# # Suppose you have some variable for each LAD code\n# set.seed(123)\n# example_data <- data.frame(\n#   lad22cd = uk_lad$LAD22CD, # local authority codes\n#   value = runif(nrow(uk_lad), 0, 100) # random values\n# )\n# \n# # --- Step 3: Join shapefile with data ---\n# uk_map <- uk_lad %>%\n#   left_join(example_data, by = c(\"LAD22CD\" = \"lad22cd\"))\n# \n# # --- Step 4: Plot heatmap ---\n# ggplot(uk_map) +\n#   geom_sf(aes(fill = value), colour = NA) +\n#   scale_fill_viridis_c(option = \"plasma\") +\n#   theme_minimal() +\n#   labs(title = \"Example Heatmap by UK Local Authority\",\n#        fill = \"Value\")"
  },
  {
    "objectID": "blog/particles/index.html",
    "href": "blog/particles/index.html",
    "title": "Some particles",
    "section": "",
    "text": "let’s dive in.\n\nWhat is particle.js\nIt’s a javascript library that draws stunning particles in a HTML document.\nYou can check it on github, and play with this little tool to find the configuration that is right for you.\n\n\nHow to use it in Quarto?\nIt is possible thanks to a “template partials”. It’s a quarto option that allows to replace the code of a section of the document.\nThe title-block partial can be used to customize the header, and inject some particles in it!\nFor more explanation, check the gallery of Quarto tips and tricks!"
  },
  {
    "objectID": "blog/post-1/index.html",
    "href": "blog/post-1/index.html",
    "title": "First Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/post-2/index.html",
    "href": "blog/post-2/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "title": "Using Neural Networks to Solve Ordinary Differential Equations",
    "section": "",
    "text": "The impressive capabilities of neural networks in typical regression and classification tasks are widely known. Recently, I read of a lesser known but mathematically very interesting application of neural networks. In fact, neural networks, can be used to numerically approximate the solutions of (ordinary) differential equations (Magdon-Ismail and Atiya 1998) . Before implementing it from scratch in Python, I will first describe the setting and the general approach."
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function",
    "title": "Post titel: ODE via NN",
    "section": "Define Custom Loss Function",
    "text": "Define Custom Loss Function\n\nclass LossODE(nn.Module):\n  def __init__(self):\n    super(LossODE, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    y = model(x)\n    \n    dy_dx = torch.autograd.grad(\n      outputs=y,\n      inputs=x,\n      grad_outputs=torch.ones_like(y),\n      create_graph=True\n      )[0]\n    \n    d2y_dx2 = torch.autograd.grad(\n      outputs=dy_dx,\n      inputs=x,\n      grad_outputs=torch.ones_like(dy_dx),\n      create_graph=True)[0]\n    \n    loss_DE = torch.mean((d2y_dx2 + 1*dy_dx + 4*y)**2)\n    loss_initial = (y[0] - 0.5)**2 + (dy_dx[0] - 2)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer",
    "title": "Post titel: ODE via NN",
    "section": "Construct model, loss and optimizer",
    "text": "Construct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossODE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values",
    "title": "Post titel: ODE via NN",
    "section": "Set x-values",
    "text": "Set x-values\n\nx = torch.linspace(0, 5, 100)[:, None]"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training",
    "title": "Post titel: ODE via NN",
    "section": "Training",
    "text": "Training\nWICHTIG: Kommentar auch in ANN für ODEs bzgl. epochs & PRINT REPLACE ÜBERNEHMEN\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.1181\n\n\nEpoch [2/10], Loss: 0.0307\n\n\nEpoch [3/10], Loss: 0.0052\n\n\nEpoch [4/10], Loss: 0.0032\n\n\nEpoch [5/10], Loss: 0.0008\n\n\nEpoch [6/10], Loss: 0.0002\n\n\nEpoch [7/10], Loss: 0.0001\n\n\nEpoch [8/10], Loss: 0.0000\n\n\nEpoch [9/10], Loss: 0.0000\n\n\nEpoch [10/10], Loss: 0.0000"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#visualize-learned-and-analytical-solution",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#visualize-learned-and-analytical-solution",
    "title": "Post titel: ODE via NN",
    "section": "Visualize learned and analytical solution",
    "text": "Visualize learned and analytical solution\n\n\nShow code\nx = torch.linspace(0, 15, 1000)[:, None]\nwith torch.no_grad():\n  y = model(x)\n\nplt.figure(figsize=(6, 5))\nplt.plot(x, y, color=\"C3\", label=\"Neural Network\")\nplt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), color=\"C0\", label=\"Analytical\", linestyle=\"dotted\")\nplt.title(\"Solution of the ODE\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function-1",
    "title": "Post titel: ODE via NN",
    "section": "Define Custom Loss Function",
    "text": "Define Custom Loss Function\n\nclass LossODE(nn.Module):\n  def __init__(self):\n    super(LossODE, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    y = model(x)\n    \n    dy_dx = torch.autograd.grad(\n      outputs=y,\n      inputs=x,\n      grad_outputs=torch.ones_like(y),\n      create_graph=True\n      )[0]\n    \n    loss_DE = torch.mean((dy_dx - 2*x*(2-y))**2)\n    loss_initial = (y[0] + 1)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seed-for-reproducibility-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seed-for-reproducibility-1",
    "title": "Post titel: ODE via NN",
    "section": "set seed for reproducibility",
    "text": "set seed for reproducibility\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x1526a662cb0>"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer-1",
    "title": "Post titel: ODE via NN",
    "section": "Construct model, loss and optimizer",
    "text": "Construct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossODE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values-1",
    "title": "Post titel: ODE via NN",
    "section": "Set x-values",
    "text": "Set x-values\n\nx = torch.linspace(0, 5, 100)[:, None]"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training-1",
    "title": "Post titel: ODE via NN",
    "section": "Training",
    "text": "Training\n\nseed = 42\ntorch.manual_seed(seed)\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.3274\n\n\nEpoch [2/10], Loss: 0.0380\n\n\nEpoch [3/10], Loss: 0.0141\n\n\nEpoch [4/10], Loss: 0.0080\n\n\nEpoch [5/10], Loss: 0.0025\n\n\nEpoch [6/10], Loss: 0.0003\n\n\nEpoch [7/10], Loss: 0.0000\n\n\nEpoch [8/10], Loss: 0.0000\n\n\nEpoch [9/10], Loss: 0.0000\n\n\nEpoch [10/10], Loss: 0.0000"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "blabla\nblubblub"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#load-packages",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#load-packages",
    "title": "Post titel: ODE via NN",
    "section": "Load Packages",
    "text": "Load Packages\n\n# for constructing, learning, using NNs\nimport torch\nimport torch.nn as nn \nimport torch.optim as optim\n\n# for visualization\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seed-for-reproducibility",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seed-for-reproducibility",
    "title": "Post titel: ODE via NN",
    "section": "Set Seed for Reproducibility",
    "text": "Set Seed for Reproducibility\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x23cea2a2610>"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seeds-for-reproducibility",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-seeds-for-reproducibility",
    "title": "Post titel: ODE via NN",
    "section": "Set Seeds for Reproducibility",
    "text": "Set Seeds for Reproducibility\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x22e69ed2cd0>"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-neural-network",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-neural-network",
    "title": "Using Neural Networks to Solve Ordinary Differential Equations",
    "section": "Define Neural Network",
    "text": "Define Neural Network\nNext, I define the neural network used for the approximation. It is a feed forward multilayer perceptron with one hidden layer that applies a \\(sigmoid\\) activation function between two fully connected linear layers.\n\nclass N(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(N, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Sigmoid(),\n      nn.Linear(hidden_size, output_size)\n      )\n    \n  def forward(self, x):\n    return self.net(x)"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#first-ode",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#first-ode",
    "title": "Using Neural Networks to Solve Ordinary Differential Equations",
    "section": "First ODE",
    "text": "First ODE\nThe first example is an IVP consisting of a second order ODE given by:\n\\[ 4 u(x) + u^{(1)}(x) + u^{(2)}(x) = 0 \\]\nand initial conditions\n\\[ u(0) = 0.5, \\; u^{(1)}(0)=2 .\\]\n\nDefine Custom Loss Function\nThe custom loss function corresponding to the above IVP is computed as:\n\nclass Loss_IVP1(nn.Module):\n  def __init__(self):\n    super(Loss_IVP1, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    u = model(x)\n    \n    # compute first derivative\n    du_dx = torch.autograd.grad(\n      outputs=u,\n      inputs=x,\n      grad_outputs=torch.ones_like(u),\n      create_graph=True\n      )[0]\n    \n    # compute second derivative\n    d2u_dx2 = torch.autograd.grad(\n      outputs=du_dx,\n      inputs=x,\n      grad_outputs=torch.ones_like(du_dx),\n      create_graph=True)[0]\n    \n    # compute loss function\n    loss_ODE = torch.mean((4*u + du_dx + d2u_dx2)**2)\n    loss_IC = (u[0] - 0.5)**2 + (du_dx[0] - 2)**2\n    loss_total = loss_ODE + loss_IC\n    \n    return loss_total\n\n\n\nConstruct Model, Loss and Optimizer\nTo ensure reproducibility of the subsequent steps, it is advisable to set a seed for the internal random number generator.\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x1f9fd8e2650>\n\n\nI instantiate the neural network, the loss function to be minimized and the optimizer (here, I use Adam).\n\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss_IVP1()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSet x-values\nI decided to approximate the solution of the IVP on the interval \\(I=[0,5]\\). I choose \\(n=100\\) sample points evenly spread accross \\(I\\).\n\nx = torch.linspace(0, 5, 100)[:, None]\n\n\n\nTraining\nNow that all necessary components are set up, training can start.\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.1181\n\n\nEpoch [2/10], Loss: 0.0307\n\n\nEpoch [3/10], Loss: 0.0052\n\n\nEpoch [4/10], Loss: 0.0032\n\n\nEpoch [5/10], Loss: 0.0008\n\n\nEpoch [6/10], Loss: 0.0002\n\n\nEpoch [7/10], Loss: 0.0001\n\n\nEpoch [8/10], Loss: 0.0000\n\n\nEpoch [9/10], Loss: 0.0000\n\n\nEpoch [10/10], Loss: 0.0000\n\n\n\n\nVisualization\nOnce training is done, the learned, approximate solution can be compared to the analytical solution, which is known for this IVP. The analytical solution is given by:\n\\[ u(x) = e^{-0.5x} \\times \\left[ 0.5 \\times \\cos\\left(\\frac{\\sqrt{15}}{2} x \\right) + \\frac{3 \\sqrt{15}}{10} \\times \\sin \\left( \\frac{\\sqrt{15}}{2} x \\right) \\right] \\]\n\n\nShow code\nx = torch.linspace(0, 15, 1000)[:, None]\nwith torch.no_grad():\n  u = model(x)\n\nplt.figure(figsize=(6, 5))\nplt.plot(x, u, color=\"C3\", label=\"Neural Network\")\nplt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), color=\"C0\", label=\"Analytical\", linestyle=\"dotted\")\nplt.title(\"Solution of the IVP\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nThe above plot shows the approximate as well as the analytical solution. Apparently, on the chosen interval \\(I=[0,5]\\) the approximate solution matches the analytical solution. Outside of \\(I\\), the approximate solution deviates from the analytical one."
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#second-ode",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#second-ode",
    "title": "Using Neural Networks to Solve Ordinary Differential Equations",
    "section": "Second ODE",
    "text": "Second ODE\nThe second example is an IVP that consists of the following first ODE\n\\[ u^{(1)}(x) - 2x (2-u) = 0 \\]\nand the initial condition\n\\[ u(0) = -1 .\\]\n\nDefine Custom Loss Function\nFor this IVP, the custom loss function is computed as:\n\nclass Loss_IVP2(nn.Module):\n  def __init__(self):\n    super(Loss_IVP2, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    u = model(x)\n    \n    # compute first derivative\n    du_dx = torch.autograd.grad(\n      outputs=u,\n      inputs=x,\n      grad_outputs=torch.ones_like(u),\n      create_graph=True\n      )[0]\n    \n    # compute loss function\n    loss_DE = torch.mean((du_dx - 2*x*(2-u))**2)\n    loss_initial = (u[0] + 1)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total\n\n\n\nConstruct Model, Loss and Optimizer\nAgain, a seed is set for the internal random number generator.\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x1f9fd8e2650>\n\n\nThe neural network, the loss function to be minimized and the optimizer are instantiated.\n\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss_IVP2()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSet x-values\nAs previously, the solution of the IVP is approximated on the interval \\(I=[0,5]\\) with \\(n=100\\) sample points evenly spread accross \\(I\\).\n\nx = torch.linspace(0, 5, 100)[:, None]\n\n\n\nTraining\nThe setup is finished and the training can begin.\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.3274\n\n\nEpoch [2/10], Loss: 0.0380\n\n\nEpoch [3/10], Loss: 0.0141\n\n\nEpoch [4/10], Loss: 0.0080\n\n\nEpoch [5/10], Loss: 0.0025\n\n\nEpoch [6/10], Loss: 0.0003\n\n\nEpoch [7/10], Loss: 0.0000\n\n\nEpoch [8/10], Loss: 0.0000\n\n\nEpoch [9/10], Loss: 0.0000\n\n\nEpoch [10/10], Loss: 0.0000\n\n\n\n\nVisualization\nFor this IVP, the analytical solution is given by:\n\\[ u(x) = 2 - 3 \\, e^{-x^2}. \\]\n\n\nShow code\nx = torch.linspace(-5, 5, 1000)[:, None]\nwith torch.no_grad():\n  u = model(x)\n\nplt.figure(figsize=(6, 5))\nplt.plot(x, u, color=\"C3\", label=\"Neural Network\")\nplt.plot(x, 2 - 3*torch.exp(-x**2), color=\"C0\" , label=\"Analytical\", linestyle=\"dotted\")\nplt.title(\"Solution of the IVP\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nAlso for the second exemplary IVP, on the interval \\(I=[0,5]\\) the approximate solution closely aligns with the analytical solution, whereas outside of it both deviate."
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#load-packages-and-set-seed",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#load-packages-and-set-seed",
    "title": "Post titel: ODE via NN",
    "section": "Load Packages and Set Seed",
    "text": "Load Packages and Set Seed\nFirst, I import the necessary packages, in particular torch for constructing neural networks, and set a seed for the random number generator to ensure reproducibility.\n\n# for constructing, learning, using NNs\nimport torch\nimport torch.nn as nn \nimport torch.optim as optim\n\n# for visualization\nimport matplotlib.pyplot as plt\n\n# set seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x21829efe610>"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#import-libraries-and-set-seed",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#import-libraries-and-set-seed",
    "title": "Post titel: ODE via NN",
    "section": "Import Libraries and Set Seed",
    "text": "Import Libraries and Set Seed\nFirst, I import the necessary libraries, in particular torch for constructing neural networks, and set a seed for the random number generator to ensure reproducibility.\n\n# for constructing, learning, using NNs\nimport torch\nimport torch.nn as nn \nimport torch.optim as optim\n\n# for visualization\nimport matplotlib.pyplot as plt\n\nseed = 42\ntorch.manual_seed(seed)\n\n<torch._C.Generator at 0x2766c5de610>"
  }
]