[
  {
    "objectID": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "href": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "title": "Dominik Eichhorn",
    "section": "",
    "text": "Load Packages\n\nimport numpy as np\nfrom scipy import stats           # sampling\n\nimport matplotlib.pyplot as plt   # visualization\n\nimport torch                      # constructing, learning, using NNs\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\nData Generation\n\n# construct 2-component Gaussian mixture distribution\ncomp1 = stats.Normal(mu=2, sigma=0.25)\ncomp2 = stats.Normal(mu=6, sigma=0.5)\nmix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])\n\n#n = 100\n#x = mix.sample(n)\n\n\n\nVisualization of the analytic PDF\n\n\n\n\n\n\n\nVisualization of Histogram\n\n\nDefine Neural Network\n\nclass Network(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(Network, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Tanh(),\n      nn.Linear(hidden_size, output_size),\n      nn.Sigmoid()\n      )\n    \n  def forward(self, x):\n    return self.net(x)\n\n\n\nDefine Custom Loss Function\n\nclass Loss(nn.Module):\n  def __init__(self):\n    super(Loss, self).__init__()\n  \n  def forward(self, y_pred, y_true):\n    return torch.mean((y_pred - y_true)**2)\n\n\nclass CustomLoss(nn.Module):\n  def __init__(self):\n    super(CustomLoss, self).__init__()\n  \n  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):\n    loss_prediction = torch.mean((y_pred - y_true)**2)\n    \n    diff = mon_l - mon_u\n    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))\n    \n    loss_total = loss_prediction + lambda_mon*loss_monotonicity\n    \n    return loss_total\n\n\n\nConstruct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = CustomLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSample data and set monotonicity points\n\n# sample data, sort it and convert to torch tensor\nn = 200\nx = mix.sample(n)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon_points = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon_points\n\n\n\nTraining\n\nnum_epochs = 200\nsteps_per_epoch = 1000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n  \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/200], Loss: 0.0182\n\n\nEpoch [2/200], Loss: 0.0111\n\n\nEpoch [3/200], Loss: 0.0064\n\n\nEpoch [4/200], Loss: 0.0041\n\n\nEpoch [5/200], Loss: 0.0050\n\n\nEpoch [6/200], Loss: 0.0073\n\n\nEpoch [7/200], Loss: 0.0054\n\n\nEpoch [8/200], Loss: 0.0038\n\n\nEpoch [9/200], Loss: 0.0052\n\n\nEpoch [10/200], Loss: 0.0033\n\n\nEpoch [11/200], Loss: 0.0047\n\n\nEpoch [12/200], Loss: 0.0057\n\n\nEpoch [13/200], Loss: 0.0036\n\n\nEpoch [14/200], Loss: 0.0061\n\n\nEpoch [15/200], Loss: 0.0046\n\n\nEpoch [16/200], Loss: 0.0075\n\n\nEpoch [17/200], Loss: 0.0057\n\n\nEpoch [18/200], Loss: 0.0057\n\n\nEpoch [19/200], Loss: 0.0048\n\n\nEpoch [20/200], Loss: 0.0062\n\n\nEpoch [21/200], Loss: 0.0046\n\n\nEpoch [22/200], Loss: 0.0038\n\n\nEpoch [23/200], Loss: 0.0039\n\n\nEpoch [24/200], Loss: 0.0038\n\n\nEpoch [25/200], Loss: 0.0046\n\n\nEpoch [26/200], Loss: 0.0047\n\n\nEpoch [27/200], Loss: 0.0039\n\n\nEpoch [28/200], Loss: 0.0058\n\n\nEpoch [29/200], Loss: 0.0047\n\n\nEpoch [30/200], Loss: 0.0048\n\n\nEpoch [31/200], Loss: 0.0033\n\n\nEpoch [32/200], Loss: 0.0022\n\n\nEpoch [33/200], Loss: 0.0030\n\n\nEpoch [34/200], Loss: 0.0029\n\n\nEpoch [35/200], Loss: 0.0024\n\n\nEpoch [36/200], Loss: 0.0038\n\n\nEpoch [37/200], Loss: 0.0026\n\n\nEpoch [38/200], Loss: 0.0022\n\n\nEpoch [39/200], Loss: 0.0032\n\n\nEpoch [40/200], Loss: 0.0016\n\n\nEpoch [41/200], Loss: 0.0023\n\n\nEpoch [42/200], Loss: 0.0016\n\n\nEpoch [43/200], Loss: 0.0017\n\n\nEpoch [44/200], Loss: 0.0018\n\n\nEpoch [45/200], Loss: 0.0019\n\n\nEpoch [46/200], Loss: 0.0013\n\n\nEpoch [47/200], Loss: 0.0024\n\n\nEpoch [48/200], Loss: 0.0029\n\n\nEpoch [49/200], Loss: 0.0020\n\n\nEpoch [50/200], Loss: 0.0013\n\n\nEpoch [51/200], Loss: 0.0024\n\n\nEpoch [52/200], Loss: 0.0022\n\n\nEpoch [53/200], Loss: 0.0010\n\n\nEpoch [54/200], Loss: 0.0010\n\n\nEpoch [55/200], Loss: 0.0021\n\n\nEpoch [56/200], Loss: 0.0018\n\n\nEpoch [57/200], Loss: 0.0012\n\n\nEpoch [58/200], Loss: 0.0012\n\n\nEpoch [59/200], Loss: 0.0015\n\n\nEpoch [60/200], Loss: 0.0016\n\n\nEpoch [61/200], Loss: 0.0036\n\n\nEpoch [62/200], Loss: 0.0016\n\n\nEpoch [63/200], Loss: 0.0035\n\n\nEpoch [64/200], Loss: 0.0016\n\n\nEpoch [65/200], Loss: 0.0006\n\n\nEpoch [66/200], Loss: 0.0019\n\n\nEpoch [67/200], Loss: 0.0019\n\n\nEpoch [68/200], Loss: 0.0019\n\n\nEpoch [69/200], Loss: 0.0036\n\n\nEpoch [70/200], Loss: 0.0013\n\n\nEpoch [71/200], Loss: 0.0008\n\n\nEpoch [72/200], Loss: 0.0009\n\n\nEpoch [73/200], Loss: 0.0009\n\n\nEpoch [74/200], Loss: 0.0017\n\n\nEpoch [75/200], Loss: 0.0010\n\n\nEpoch [76/200], Loss: 0.0011\n\n\nEpoch [77/200], Loss: 0.0012\n\n\nEpoch [78/200], Loss: 0.0013\n\n\nEpoch [79/200], Loss: 0.0011\n\n\nEpoch [80/200], Loss: 0.0019\n\n\nEpoch [81/200], Loss: 0.0007\n\n\nEpoch [82/200], Loss: 0.0019\n\n\nEpoch [83/200], Loss: 0.0013\n\n\nEpoch [84/200], Loss: 0.0014\n\n\nEpoch [85/200], Loss: 0.0010\n\n\nEpoch [86/200], Loss: 0.0030\n\n\nEpoch [87/200], Loss: 0.0007\n\n\nEpoch [88/200], Loss: 0.0014\n\n\nEpoch [89/200], Loss: 0.0016\n\n\nEpoch [90/200], Loss: 0.0009\n\n\nEpoch [91/200], Loss: 0.0029\n\n\nEpoch [92/200], Loss: 0.0009\n\n\nEpoch [93/200], Loss: 0.0003\n\n\nEpoch [94/200], Loss: 0.0008\n\n\nEpoch [95/200], Loss: 0.0040\n\n\nEpoch [96/200], Loss: 0.0009\n\n\nEpoch [97/200], Loss: 0.0026\n\n\nEpoch [98/200], Loss: 0.0011\n\n\nEpoch [99/200], Loss: 0.0005\n\n\nEpoch [100/200], Loss: 0.0009\n\n\nEpoch [101/200], Loss: 0.0012\n\n\nEpoch [102/200], Loss: 0.0005\n\n\nEpoch [103/200], Loss: 0.0030\n\n\nEpoch [104/200], Loss: 0.0012\n\n\nEpoch [105/200], Loss: 0.0009\n\n\nEpoch [106/200], Loss: 0.0012\n\n\nEpoch [107/200], Loss: 0.0006\n\n\nEpoch [108/200], Loss: 0.0013\n\n\nEpoch [109/200], Loss: 0.0041\n\n\nEpoch [110/200], Loss: 0.0015\n\n\nEpoch [111/200], Loss: 0.0025\n\n\nEpoch [112/200], Loss: 0.0008\n\n\nEpoch [113/200], Loss: 0.0008\n\n\nEpoch [114/200], Loss: 0.0005\n\n\nEpoch [115/200], Loss: 0.0014\n\n\nEpoch [116/200], Loss: 0.0018\n\n\nEpoch [117/200], Loss: 0.0017\n\n\nEpoch [118/200], Loss: 0.0013\n\n\nEpoch [119/200], Loss: 0.0019\n\n\nEpoch [120/200], Loss: 0.0009\n\n\nEpoch [121/200], Loss: 0.0024\n\n\nEpoch [122/200], Loss: 0.0010\n\n\nEpoch [123/200], Loss: 0.0011\n\n\nEpoch [124/200], Loss: 0.0006\n\n\nEpoch [125/200], Loss: 0.0005\n\n\nEpoch [126/200], Loss: 0.0010\n\n\nEpoch [127/200], Loss: 0.0010\n\n\nEpoch [128/200], Loss: 0.0054\n\n\nEpoch [129/200], Loss: 0.0006\n\n\nEpoch [130/200], Loss: 0.0007\n\n\nEpoch [131/200], Loss: 0.0010\n\n\nEpoch [132/200], Loss: 0.0005\n\n\nEpoch [133/200], Loss: 0.0013\n\n\nEpoch [134/200], Loss: 0.0005\n\n\nEpoch [135/200], Loss: 0.0010\n\n\nEpoch [136/200], Loss: 0.0027\n\n\nEpoch [137/200], Loss: 0.0006\n\n\nEpoch [138/200], Loss: 0.0010\n\n\nEpoch [139/200], Loss: 0.0014\n\n\nEpoch [140/200], Loss: 0.0008\n\n\nEpoch [141/200], Loss: 0.0006\n\n\nEpoch [142/200], Loss: 0.0017\n\n\nEpoch [143/200], Loss: 0.0012\n\n\nEpoch [144/200], Loss: 0.0003\n\n\nEpoch [145/200], Loss: 0.0005\n\n\nEpoch [146/200], Loss: 0.0011\n\n\nEpoch [147/200], Loss: 0.0010\n\n\nEpoch [148/200], Loss: 0.0007\n\n\nEpoch [149/200], Loss: 0.0007\n\n\nEpoch [150/200], Loss: 0.0019\n\n\nEpoch [151/200], Loss: 0.0007\n\n\nEpoch [152/200], Loss: 0.0010\n\n\nEpoch [153/200], Loss: 0.0009\n\n\nEpoch [154/200], Loss: 0.0023\n\n\nEpoch [155/200], Loss: 0.0018\n\n\nEpoch [156/200], Loss: 0.0004\n\n\nEpoch [157/200], Loss: 0.0006\n\n\nEpoch [158/200], Loss: 0.0010\n\n\nEpoch [159/200], Loss: 0.0019\n\n\nEpoch [160/200], Loss: 0.0022\n\n\nEpoch [161/200], Loss: 0.0004\n\n\nEpoch [162/200], Loss: 0.0008\n\n\nEpoch [163/200], Loss: 0.0010\n\n\nEpoch [164/200], Loss: 0.0017\n\n\nEpoch [165/200], Loss: 0.0007\n\n\nEpoch [166/200], Loss: 0.0005\n\n\nEpoch [167/200], Loss: 0.0007\n\n\nEpoch [168/200], Loss: 0.0012\n\n\nEpoch [169/200], Loss: 0.0032\n\n\nEpoch [170/200], Loss: 0.0005\n\n\nEpoch [171/200], Loss: 0.0013\n\n\nEpoch [172/200], Loss: 0.0016\n\n\nEpoch [173/200], Loss: 0.0009\n\n\nEpoch [174/200], Loss: 0.0074\n\n\nEpoch [175/200], Loss: 0.0020\n\n\nEpoch [176/200], Loss: 0.0007\n\n\nEpoch [177/200], Loss: 0.0022\n\n\nEpoch [178/200], Loss: 0.0029\n\n\nEpoch [179/200], Loss: 0.0004\n\n\nEpoch [180/200], Loss: 0.0020\n\n\nEpoch [181/200], Loss: 0.0009\n\n\nEpoch [182/200], Loss: 0.0020\n\n\nEpoch [183/200], Loss: 0.0009\n\n\nEpoch [184/200], Loss: 0.0010\n\n\nEpoch [185/200], Loss: 0.0009\n\n\nEpoch [186/200], Loss: 0.0009\n\n\nEpoch [187/200], Loss: 0.0011\n\n\nEpoch [188/200], Loss: 0.0005\n\n\nEpoch [189/200], Loss: 0.0013\n\n\nEpoch [190/200], Loss: 0.0016\n\n\nEpoch [191/200], Loss: 0.0008\n\n\nEpoch [192/200], Loss: 0.0011\n\n\nEpoch [193/200], Loss: 0.0029\n\n\nEpoch [194/200], Loss: 0.0004\n\n\nEpoch [195/200], Loss: 0.0007\n\n\nEpoch [196/200], Loss: 0.0022\n\n\nEpoch [197/200], Loss: 0.0006\n\n\nEpoch [198/200], Loss: 0.0006\n\n\nEpoch [199/200], Loss: 0.0014\n\n\nEpoch [200/200], Loss: 0.0117\n\n\n\nimport matplotlib.pyplot as plt\nxx = torch.linspace(0, 10, 500).unsqueeze(1)\nwith torch.no_grad():\n    yy = model(xx)\n\nplt.figure(figsize=(10, 5))\n#plt.plot(x.numpy(), u.numpy(), label=\"Target (sorted uniform)\", color='green')\nplt.plot(xx.numpy(), yy.numpy(), label=\"Learned CDF\", color='blue')\nplt.plot(xx.numpy(), mix.cdf(xx), label=\"Learned CDF\", color='red')\nplt.title(\"NN fitting empirical CDF\")\nplt.legend()\nplt.show()\n\n\n\n\n\n# Define a function to compute derivative of the NN output wrt input\ndef compute_pdf_from_cdf(model, x):\n    x = x.requires_grad_(True)  # enable grad w.r.t x\n    cdf = model(x)\n    grad_outputs = torch.ones_like(cdf)\n    pdf = torch.autograd.grad(\n        outputs=cdf,\n        inputs=x,\n        grad_outputs=grad_outputs,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    return pdf\n\n# Create a grid of points where we want to evaluate PDF\nxx = torch.linspace(0, 10, 100).unsqueeze(1)\n\n# Compute CDF and PDF estimates\nwith torch.no_grad():\n    cdf_est = model(xx)\n\npdf_est = compute_pdf_from_cdf(model, xx).detach()\n\n# Compute true PDF from the mixture for comparison\nxx_np = xx.detach().numpy().flatten()\ntrue_pdf = mix.pdf(xx_np)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(xx_np, pdf_est.numpy(), label='NN PDF Estimate', color='blue')\nplt.plot(xx_np, true_pdf, label='True PDF', color='red', linestyle='dashed')\nplt.title(\"PDF estimated by derivative of NN CDF vs True PDF\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "title": "Dominik Eichhorn",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.hidden_layer = nn.Linear(1, 10)\n    self.output_layer = nn.Linear(10, 1)\n    \n  def forward(self, x):\n    layer_out = torch.sigmoid(self.hidden_layer(x))\n    output = self.output_layer(layer_out)\n    return output\n    \nN = Network()\nN = N.to(device)\n\ndef f(x):\n    return torch.exp(x)\n\ndef loss(x):\n    x.requires_grad = True\n    y = N(x)\n    dy_dx = torch.autograd.grad(y.sum(), x, create_graph=True)[0]\n    d2y_dx2 = torch.autograd.grad(dy_dx, x,\n                                  grad_outputs=torch.ones_like(dy_dx),\n                                  create_graph=True)[0]\n    #return torch.mean( (d2y_dx2 + 2*dy_dx +4*y)**2 ) + (y[0, 0] - 1)**2 + (dy_dx[0, 0] - 3)**2\n    return torch.mean( (d2y_dx2 + 1*dy_dx +4*y)**2 ) + (y[0] - 0.5)**2 + (dy_dx[0] - 2)**2\n  \n\n\noptimizer = torch.optim.LBFGS(N.parameters())\n\nx = torch.linspace(0, 5, 100)[:, None]\n\ndef closure():\n    optimizer.zero_grad()\n    l = loss(x)\n    l.backward()\n    return l\n\nepochs = 10\nfor i in range(epochs):\n    optimizer.step(closure)\n    \n    \n\nimport matplotlib.pyplot as plt\n\nxx = torch.linspace(0, 15, 100)[:, None]\nwith torch.no_grad():\n    yy = N(xx)\n\nplt.figure(figsize=(10, 6))\nplt.plot(xx, yy, label=\"Predicted\")\nplt.plot(xx, torch.exp(-0.5*xx)*(0.5*torch.cos(xx*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(xx*(15)**0.5/2)), '--', label=\"Exact\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Network(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.hidden_layer = nn.Linear(1, 10)\n    self.output_layer = nn.Linear(10, 1)\n    \n  def forward(self, x):\n    layer_out = torch.sigmoid(self.hidden_layer(x))\n    output = self.output_layer(layer_out)\n    return output\n    \nN = Network()\nN = N.to(device)\n\ndef f(x):\n    return torch.exp(x)\n\ndef loss(x):\n    x.requires_grad = True\n    y = N(x)\n    dy_dx = torch.autograd.grad(y.sum(), x, create_graph=True)[0]\n    return torch.mean( (dy_dx - f(x))**2 ) + (y[0, 0] - 1.)**2\n  \n\n\noptimizer = torch.optim.LBFGS(N.parameters())\n\nx = torch.linspace(0, 1, 100)[:, None]\n\ndef closure():\n    optimizer.zero_grad()\n    l = loss(x)\n    l.backward()\n    return l\n\nepochs = 10\nfor i in range(epochs):\n    optimizer.step(closure)\n    \n    \n\nimport matplotlib.pyplot as plt\n\nxx = torch.linspace(0, 1, 100)[:, None]\nwith torch.no_grad():\n    yy = N(xx)\n\nplt.figure(figsize=(10, 6))\nplt.plot(xx, yy, label=\"Predicted\")\nplt.plot(xx, torch.exp(xx), '--', label=\"Exact\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n# import torch\n# import torch.nn as nn\n# import matplotlib.pyplot as plt\n# \n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# \n# # Define neural network\n# class Network(nn.Module):\n#   def __init__(self):\n#     super().__init__()\n#     self.hidden_layer = nn.Linear(1, 10)\n#     self.output_layer = nn.Linear(10, 1)\n# \n#   def forward(self, x):\n#     x = torch.sigmoid(self.hidden_layer(x))\n#     return self.output_layer(x)\n# \n# # Initialize network\n# N = Network().to(device)\n# \n# # Trial solution: y_trial(x) = 1 + x * 0 + x^2 * N(x)\n# # (satisfies y(0)=1, y'(0)=0)\n# def y_trial(x):\n#   x = x.clone().detach().requires_grad_(True).to(device)\n#   net_out = N(x)\n#   return 1 + x**2 * net_out\n# \n# # Loss function for y''(x) - y(x) = 0\n# def loss_fn(x):\n#   x = x.clone().detach().requires_grad_(True).to(device)\n#   y = y_trial(x)\n# \n#   dy_dx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]\n#   d2y_dx2 = torch.autograd.grad(dy_dx, x, grad_outputs=torch.ones_like(dy_dx), create_graph=True)[0]\n# \n#   residual = d2y_dx2 - y\n#   return torch.mean(residual**2)\n# \n# # Training setup\n# x = torch.linspace(0, 1, 100).view(-1, 1).to(device)\n# optimizer = torch.optim.LBFGS(N.parameters())\n# \n# def closure():\n#   optimizer.zero_grad()\n#   loss = loss_fn(x)\n#   loss.backward()\n#   return loss\n# \n# # Train\n# epochs = 10\n# for epoch in range(epochs):\n#   optimizer.step(closure)\n#   print(f\"Epoch {epoch}, Loss: {loss_fn(x).item()}\")\n# \n# # Evaluate and plot\n# with torch.no_grad():\n#   xx = torch.linspace(0, 1, 100).view(-1, 1).to(device)\n#   yy = y_trial(xx)\n# \n# plt.figure(figsize=(10, 6))\n# plt.plot(xx.cpu(), yy.cpu(), label=\"Predicted\")\n# plt.plot(xx.cpu(), torch.cosh(xx.cpu()), '--', label=\"Exact (cosh)\")\n# plt.xlabel(\"x\")\n# plt.ylabel(\"y\")\n# plt.title(\"Second-Order ODE: $y'' = y$, $y(0)=1$, $y'(0)=0$\")\n# plt.legend()\n# plt.grid(True)\n# plt.show()"
  },
  {
    "objectID": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "href": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "title": "Dominik Eichhorn",
    "section": "",
    "text": "Daten können unter diesem Link downgeloaded werden: “https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip”\n\ngetwd()\n\n[1] \"C:/Users/domin/Documents/PersonalWebsite/dgeichhorn.github.io/blog/CopulasForFF5Factors\"\n\nlibrary(readr)\ndt <- read_csv(\"F-F_Research_Data_5_Factors_2x3.csv\",\n               skip = 4, n_max = 743,\n               col_names = c(\"Date\", \"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"))\n\nRows: 743 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): Date, Mkt_RF, SMB, HML, RMW, CMA, RF\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(dt)\n\ncor_matrix <- cor(dt[, c(\"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\")])\nprint(cor_matrix)\n\n           Mkt_RF          SMB          HML          RMW          CMA\nMkt_RF  1.0000000  0.277162682 -0.205902792 -0.186881612 -0.353833434\nSMB     0.2771627  1.000000000  0.006444189 -0.348613074 -0.087387014\nHML    -0.2059028  0.006444189  1.000000000  0.085160500  0.682067617\nRMW    -0.1868816 -0.348613074  0.085160500  1.000000000 -0.003665752\nCMA    -0.3538334 -0.087387014  0.682067617 -0.003665752  1.000000000\n\nplot(dt$HML, dt$CMA)"
  },
  {
    "objectID": "blog/LearningToRank/LearningToRank.html",
    "href": "blog/LearningToRank/LearningToRank.html",
    "title": "Learning to Rank",
    "section": "",
    "text": "library(BradleyTerry2)\n\ndata(citations)\n\nteam.names <- c(\"FCN\", \"VfB\", \"BVB\", \"HSV\")\n\nW <- matrix(\n  c(0, 1, 0, 0,\n    0, 0, 1, 0,\n    1, 0, 0, 1,\n    5, 4, 4, 0),\n  nrow = 4,\n  byrow = TRUE,\n  dimnames = list(\n    team.names,  # Row names\n    team.names   # Column names\n  )\n)\n\n# number of teams\np <- dim(W)[1]\n\ntheta.old <- rep(1, p)\ntheta.new <- numeric(p)\n\nfor (k in 1:100) {\n  \n  for (i in 1:p) {\n    theta.new[i] <- sum(W[i,])/(sum((W[i,] + W[,i])/(theta.old[i]+theta.old)))\n  }\n  \n  # divide by geomtric mean\n  theta.new <- theta.new/prod(theta.new)^(1/p)\n  \n  theta.old <- theta.new\n}\n\n# wahrsch dass FCN gegen VfB gewinnt: \n1.042565/(1.042565+ 0.795519)\n\n[1] 0.567202\n\nb<-countsToBinomial(W)\n\ncitation_matrix <- matrix(\n  c(714, 730, 498, 221,    # Biometrika\n    33, 425, 68, 17,       # Comm Statist\n    320, 813, 1072, 142,   # JASA\n    284, 276, 325, 188),   # JRSS-B\n  nrow = 4,\n  byrow = TRUE\n)\n\n# Assign row and column names\nrownames(citation_matrix) <- c(\"Biometrika\", \"Comm Statist\", \"JASA\", \"JRSS-B\")\ncolnames(citation_matrix) <- c(\"Biometrika\", \"Comm Statist\", \"JASA\", \"JRSS-B\")\n\ncitations.sf <- countsToBinomial(citations)\n\na<-countsToBinomial(citation_matrix)\n\n\n\n\ncitations.sf <- countsToBinomial(W)\nnames(citations.sf)[1:2] <- c(\"journal1\", \"journal2\")\n## Fit the \"standard\" Bradley-Terry model\nciteModel <- BTm(cbind(win1, win2), journal1, journal2, data = citations.sf)\nBTabilities(citeModel)\n\n       ability     s.e.\nFCN 0.00000000 0.000000\nVfB 0.05301869 1.484210\nBVB 0.95329538 1.450889\nHSV 3.00933786 1.436988\n\n# Converting my values into BTm ability values\nlog(theta.old/theta.old[1])\n\n[1] 0.00000000 0.05301869 0.95329538 3.00933787\n\n# Converting BTm values to my values\nBTabilities <- BTabilities(citeModel)[,1]\nexp(BTabilities)*theta.old[1]\n\n      FCN       VfB       BVB       HSV \n0.3664427 0.3863953 0.9506421 7.4292498 \n\n\n\nd <- 0.85\n\nr <- rep(1/p, p)\n\n# W_ij: number of times team j lost against team i\nW\n\n    FCN VfB BVB HSV\nFCN   0   1   0   0\nVfB   0   0   1   0\nBVB   1   0   0   1\nHSV   5   4   4   0\n\n# c_j: number of times team j lost (=colSums of W)\n\nc <- colSums(W)\n\nA <- (1-d)*rep(1,p) %*% t(rep(1,p))/p + d*W%*%diag(1/c) \n\nfor (i in 1:100) {\n  print(r)\n  r <- A%*%r\n}\n\n[1] 0.25 0.25 0.25 0.25\n         [,1]\nFCN 0.0800000\nVfB 0.0800000\nBVB 0.2854167\nHSV 0.5545833\n          [,1]\nFCN 0.05110000\nVfB 0.08602083\nBVB 0.52022917\nHSV 0.34265000\n          [,1]\nFCN 0.05212354\nVfB 0.12593896\nBVB 0.33599167\nHSV 0.48594583\n          [,1]\nFCN 0.05890962\nVfB 0.09461858\nBVB 0.45793813\nHSV 0.38853367\n          [,1]\nFCN 0.05358516\nVfB 0.11534948\nBVB 0.37609915\nHSV 0.45496621\n          [,1]\nFCN 0.05710941\nVfB 0.10143685\nBVB 0.43181251\nHSV 0.40964122\n          [,1]\nFCN 0.05474427\nVfB 0.11090813\nBVB 0.39378554\nHSV 0.44056207\n          [,1]\nFCN 0.05635438\nVfB 0.10444354\nBVB 0.41973320\nHSV 0.41946888\n         [,1]\nFCN 0.0552554\nVfB 0.1088546\nBVB 0.4020321\nHSV 0.4338579\n          [,1]\nFCN 0.05600529\nVfB 0.10584545\nBVB 0.41410704\nHSV 0.42404222\n          [,1]\nFCN 0.05549373\nVfB 0.10789820\nBVB 0.40586997\nHSV 0.43073811\n          [,1]\nFCN 0.05584269\nVfB 0.10649789\nBVB 0.41148900\nHSV 0.42617041\n          [,1]\nFCN 0.05560464\nVfB 0.10745313\nBVB 0.40765590\nHSV 0.42928633\n          [,1]\nFCN 0.05576703\nVfB 0.10680150\nBVB 0.41027071\nHSV 0.42716076\n          [,1]\nFCN 0.05565626\nVfB 0.10724602\nBVB 0.40848698\nHSV 0.42861075\n          [,1]\nFCN 0.05573182\nVfB 0.10694279\nBVB 0.40970377\nHSV 0.42762162\n          [,1]\nFCN 0.05568027\nVfB 0.10714964\nBVB 0.40887372\nHSV 0.42829637\n          [,1]\nFCN 0.05571544\nVfB 0.10700853\nBVB 0.40943995\nHSV 0.42783608\n          [,1]\nFCN 0.05569145\nVfB 0.10710479\nBVB 0.40905369\nHSV 0.42815007\n          [,1]\nFCN 0.05570781\nVfB 0.10703913\nBVB 0.40931718\nHSV 0.42793588\n          [,1]\nFCN 0.05569665\nVfB 0.10708392\nBVB 0.40913743\nHSV 0.42808199\n          [,1]\nFCN 0.05570427\nVfB 0.10705336\nBVB 0.40926005\nHSV 0.42798232\n          [,1]\nFCN 0.05569907\nVfB 0.10707421\nBVB 0.40917641\nHSV 0.42805031\n          [,1]\nFCN 0.05570262\nVfB 0.10705999\nBVB 0.40923347\nHSV 0.42800393\n         [,1]\nFCN 0.0557002\nVfB 0.1070697\nBVB 0.4091945\nHSV 0.4280356\n          [,1]\nFCN 0.05570185\nVfB 0.10706307\nBVB 0.40922110\nHSV 0.42801398\n          [,1]\nFCN 0.05570072\nVfB 0.10706759\nBVB 0.40920298\nHSV 0.42802871\n          [,1]\nFCN 0.05570149\nVfB 0.10706451\nBVB 0.40921534\nHSV 0.42801866\n          [,1]\nFCN 0.05570097\nVfB 0.10706661\nBVB 0.40920691\nHSV 0.42802552\n          [,1]\nFCN 0.05570132\nVfB 0.10706517\nBVB 0.40921266\nHSV 0.42802084\n          [,1]\nFCN 0.05570108\nVfB 0.10706615\nBVB 0.40920874\nHSV 0.42802403\n          [,1]\nFCN 0.05570125\nVfB 0.10706549\nBVB 0.40921141\nHSV 0.42802186\n          [,1]\nFCN 0.05570113\nVfB 0.10706594\nBVB 0.40920959\nHSV 0.42802334\n          [,1]\nFCN 0.05570121\nVfB 0.10706563\nBVB 0.40921083\nHSV 0.42802233\n          [,1]\nFCN 0.05570116\nVfB 0.10706584\nBVB 0.40920998\nHSV 0.42802302\n          [,1]\nFCN 0.05570119\nVfB 0.10706570\nBVB 0.40921056\nHSV 0.42802255\n          [,1]\nFCN 0.05570117\nVfB 0.10706580\nBVB 0.40921017\nHSV 0.42802287\n          [,1]\nFCN 0.05570119\nVfB 0.10706573\nBVB 0.40921044\nHSV 0.42802265\n          [,1]\nFCN 0.05570117\nVfB 0.10706577\nBVB 0.40921025\nHSV 0.42802280\n          [,1]\nFCN 0.05570118\nVfB 0.10706574\nBVB 0.40921038\nHSV 0.42802270\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921029\nHSV 0.42802277\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921035\nHSV 0.42802272\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921031\nHSV 0.42802275\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921034\nHSV 0.42802273\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921032\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921033\nHSV 0.42802273\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921032\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n\nlibrary(igraph)\n\nWarning: Paket 'igraph' wurde unter R Version 4.2.3 erstellt\n\n\n\nAttache Paket: 'igraph'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    decompose, spectrum\n\n\nDas folgende Objekt ist maskiert 'package:base':\n\n    union\n\ngraphObj <- graph_from_adjacency_matrix(t(W), weighted = TRUE, mode = \"directed\")\n(prVec <- page_rank(graphObj)$vector)\n\n       FCN        VfB        BVB        HSV \n0.05570118 0.10706576 0.40921033 0.42802274 \n\n# loss matrix: L_ij is number of times team i lost against team j\n#L <- t(W)\n\n\n# normalize cols to make it a column stochastic matrix\n#A <- sweep(L, 2, colSums(L), FUN = \"/\")\n\n\n\n#for (i in 1:100) {\n#    r <- d * (A %*% r) + (1 - d) * 1/p\n#    #if (sum(abs(r_new - r)) < tol) {\n#    #  break\n#    #}\n#    #r <- r_new\n#    print(r)\n#  }\n\n\n# link wo man daten als csv bekommt: https://www.hockey-reference.com/leagues/NHL_2025_games.html#games\n\nlibrary(rvest)\n\nWarning: Paket 'rvest' wurde unter R Version 4.2.3 erstellt\n\nlibrary(dplyr)\n\nWarning: Paket 'dplyr' wurde unter R Version 4.2.3 erstellt\n\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:igraph':\n\n    as_data_frame, groups, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# URL of the schedule page\nurl <- \"https://www.hockey-reference.com/leagues/NHL_2025_games.html\"\n\n# Read the webpage\npage <- read_html(url)\n\n# Extract the Regular Season Schedule table (it has id=\"games\")\nschedule <- page %>%\n  html_node(\"table#games\") %>%\n  html_table(header = TRUE, fill = TRUE)\n\n# Clean up data as needed\n#schedule_clean <- schedule %>%\n#  # Convert Date column to Date type\n#  mutate(Date = as.Date(Date)) %>%\n#  # Clean Time (e.g. add timezone if needed), convert numeric columns\n#  mutate(\n#    Visitor = Visitor,\n#    Home = Home,\n#    `Visitor G` = as.integer(`Visitor G`),\n#    `Home G` = as.integer(`Home G`),\n#    Att. = as.integer(gsub(\",\", \"\", Att.)),\n#    Time = Time\n#  )\n\n# View the result\n#str(schedule_clean)\n#head(schedule_clean)"
  },
  {
    "objectID": "blog/post-1/index.html",
    "href": "blog/post-1/index.html",
    "title": "First Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/post-2/index.html",
    "href": "blog/post-2/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "blabla\nblubblub"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Post description for first post\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this post, I implement from scratch two different approaches for learning a ranking from data on the results of pairwise comparisons.\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\nPost description for second post\n\n\n\n\n\n\nMay 23, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "J. T. Smith",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nso mal sehen"
  }
]