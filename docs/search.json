[
  {
    "objectID": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "href": "blog/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "title": "Dominik Eichhorn",
    "section": "",
    "text": "Load Packages\n\n# for sampling\nimport random\nimport numpy as np\nfrom scipy import stats\n\n# for visualization\nimport matplotlib.pyplot as plt\n\n# for constructing, learning, using NNs\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\nData Generation\n\n# construct 2-component Gaussian mixture distribution\ncomp1 = stats.Normal(mu=2, sigma=0.25)\ncomp2 = stats.Normal(mu=6, sigma=0.5)\nmix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])\n\n\n\nVisualization of the analytic PDF\n\n\nShow code\nx = np.linspace(0, 10, 1000)\n\nplt.figure(figsize=(8,5))\nplt.plot(x, mix.pdf(x), color=\"C0\", linestyle=\"dashed\")\nplt.title(\"True Probability Density Function\")\nplt.xlabel(\"Density\")\nplt.ylabel(\"x\")\nplt.show()\n\n\n\n\n\n\n\nVisualization of Histogram\n\n\nDefine Neural Network\n\nclass Network(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(Network, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Tanh(),\n      nn.Linear(hidden_size, output_size),\n      nn.Sigmoid()\n      )\n    \n  def forward(self, x):\n    return self.net(x)\n\n\n\nDefine Custom Loss Function\n\nclass LossPDE(nn.Module):\n  def __init__(self):\n    super(LossPDE, self).__init__()\n  \n  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):\n    loss_prediction = torch.mean((y_pred - y_true)**2)\n    \n    diff = mon_l - mon_u\n    loss_monotonicity = torch.mean(torch.clamp(diff, min=0))\n    \n    loss_total = loss_prediction + lambda_mon*loss_monotonicity\n    \n    return loss_total\n\n\n\nConstruct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossPDE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSample data and set monotonicity points\n\n# sample data, sort it and convert to torch tensor\nn = 200\nrng = np.random.default_rng(42)\nx = mix.sample(n, rng=rng)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon_points = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon_points)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon_points\n\n\n\nTraining\nwichtiger kommentar: in jedem step wird ganzer datensatz verwendet; epochs dienen nur dazu dass nicht in jedem step fortschritt gedruck wird\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nnum_epochs = 200\nsteps_per_epoch = 1000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\", end=\"\\r\", flush=True)\n\n\n\nShow code\nxx = torch.linspace(0, 10, 1000).unsqueeze(1)\nwith torch.no_grad():\n    cdf_est = model(xx)\n\nplt.figure(figsize=(8, 5))\nplt.plot(xx.numpy(), cdf_est.numpy(), color=\"C3\", label=\"Estimated CDF\")\nplt.plot(xx.numpy(), mix.cdf(xx), color=\"C0\", label=\"True CDF\", linestyle=\"dashed\")\nplt.title(\"Cumulative Density Function (CDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Cumulative Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFunction to compute derivate of NN w.r.t. to input x\n\ndef ComputePDF(model, x):\n  x = x.requires_grad_(True)\n  cdf = model(x)\n  grad_outputs = torch.ones_like(cdf)\n  pdf = torch.autograd.grad(\n    outputs=cdf,\n    inputs=x,\n    grad_outputs=grad_outputs\n    )[0]\n  return pdf\n\n\n\nPlot PDF\n\n\nShow code\npdf_est = ComputePDF(model, xx).detach()\nxx_np = xx.detach().numpy()\n\n\nplt.figure(figsize=(8, 5))\nplt.plot(xx_np, pdf_est.numpy(), color=\"C3\", label=\"Estimated PDF\")\nplt.plot(xx_np, mix.pdf(xx_np), color=\"C0\", label=\"True PDF\", linestyle='dashed')\nplt.title(\"Probability Density Function (PDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "",
    "text": "nach zweiter LossODE: HIDE code show plots\n\nconstruct model, criterion and loss\ntraining\nvisualization with analyticaly solution"
  },
  {
    "objectID": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "href": "blog/CopulasForFF5Factors/CopulasForFF5Factors.html",
    "title": "Dominik Eichhorn",
    "section": "",
    "text": "Daten können unter diesem Link downgeloaded werden: “https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip”\n\ngetwd()\n\n[1] \"C:/Users/domin/Documents/PersonalWebsite/dgeichhorn.github.io/blog/CopulasForFF5Factors\"\n\nlibrary(readr)\ndt <- read_csv(\"F-F_Research_Data_5_Factors_2x3.csv\",\n               skip = 4, n_max = 743,\n               col_names = c(\"Date\", \"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"))\n\nRows: 743 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): Date, Mkt_RF, SMB, HML, RMW, CMA, RF\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nView(dt)\n\ncor_matrix <- cor(dt[, c(\"Mkt_RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\")])\nprint(cor_matrix)\n\n           Mkt_RF          SMB          HML          RMW          CMA\nMkt_RF  1.0000000  0.277162682 -0.205902792 -0.186881612 -0.353833434\nSMB     0.2771627  1.000000000  0.006444189 -0.348613074 -0.087387014\nHML    -0.2059028  0.006444189  1.000000000  0.085160500  0.682067617\nRMW    -0.1868816 -0.348613074  0.085160500  1.000000000 -0.003665752\nCMA    -0.3538334 -0.087387014  0.682067617 -0.003665752  1.000000000\n\nplot(dt$HML, dt$CMA)"
  },
  {
    "objectID": "blog/LearningToRank/LearningToRank.html",
    "href": "blog/LearningToRank/LearningToRank.html",
    "title": "Learning to Rank",
    "section": "",
    "text": "library(BradleyTerry2)\n\ndata(citations)\n\nteam.names <- c(\"FCN\", \"VfB\", \"BVB\", \"HSV\")\n\nW <- matrix(\n  c(0, 1, 0, 0,\n    0, 0, 1, 0,\n    1, 0, 0, 1,\n    5, 4, 4, 0),\n  nrow = 4,\n  byrow = TRUE,\n  dimnames = list(\n    team.names,  # Row names\n    team.names   # Column names\n  )\n)\n\n# number of teams\np <- dim(W)[1]\n\ntheta.old <- rep(1, p)\ntheta.new <- numeric(p)\n\nfor (k in 1:100) {\n  \n  for (i in 1:p) {\n    theta.new[i] <- sum(W[i,])/(sum((W[i,] + W[,i])/(theta.old[i]+theta.old)))\n  }\n  \n  # divide by geomtric mean\n  theta.new <- theta.new/prod(theta.new)^(1/p)\n  \n  theta.old <- theta.new\n}\n\n# wahrsch dass FCN gegen VfB gewinnt: \n1.042565/(1.042565+ 0.795519)\n\n[1] 0.567202\n\nb<-countsToBinomial(W)\n\ncitation_matrix <- matrix(\n  c(714, 730, 498, 221,    # Biometrika\n    33, 425, 68, 17,       # Comm Statist\n    320, 813, 1072, 142,   # JASA\n    284, 276, 325, 188),   # JRSS-B\n  nrow = 4,\n  byrow = TRUE\n)\n\n# Assign row and column names\nrownames(citation_matrix) <- c(\"Biometrika\", \"Comm Statist\", \"JASA\", \"JRSS-B\")\ncolnames(citation_matrix) <- c(\"Biometrika\", \"Comm Statist\", \"JASA\", \"JRSS-B\")\n\ncitations.sf <- countsToBinomial(citations)\n\na<-countsToBinomial(citation_matrix)\n\n\n\n\ncitations.sf <- countsToBinomial(W)\nnames(citations.sf)[1:2] <- c(\"journal1\", \"journal2\")\n## Fit the \"standard\" Bradley-Terry model\nciteModel <- BTm(cbind(win1, win2), journal1, journal2, data = citations.sf)\nBTabilities(citeModel)\n\n       ability     s.e.\nFCN 0.00000000 0.000000\nVfB 0.05301869 1.484210\nBVB 0.95329538 1.450889\nHSV 3.00933786 1.436988\n\n# Converting my values into BTm ability values\nlog(theta.old/theta.old[1])\n\n[1] 0.00000000 0.05301869 0.95329538 3.00933787\n\n# Converting BTm values to my values\nBTabilities <- BTabilities(citeModel)[,1]\nexp(BTabilities)*theta.old[1]\n\n      FCN       VfB       BVB       HSV \n0.3664427 0.3863953 0.9506421 7.4292498 \n\n\n\nd <- 0.85\n\nr <- rep(1/p, p)\n\n# W_ij: number of times team j lost against team i\nW\n\n    FCN VfB BVB HSV\nFCN   0   1   0   0\nVfB   0   0   1   0\nBVB   1   0   0   1\nHSV   5   4   4   0\n\n# c_j: number of times team j lost (=colSums of W)\n\nc <- colSums(W)\n\nA <- (1-d)*rep(1,p) %*% t(rep(1,p))/p + d*W%*%diag(1/c) \n\nfor (i in 1:100) {\n  print(r)\n  r <- A%*%r\n}\n\n[1] 0.25 0.25 0.25 0.25\n         [,1]\nFCN 0.0800000\nVfB 0.0800000\nBVB 0.2854167\nHSV 0.5545833\n          [,1]\nFCN 0.05110000\nVfB 0.08602083\nBVB 0.52022917\nHSV 0.34265000\n          [,1]\nFCN 0.05212354\nVfB 0.12593896\nBVB 0.33599167\nHSV 0.48594583\n          [,1]\nFCN 0.05890962\nVfB 0.09461858\nBVB 0.45793813\nHSV 0.38853367\n          [,1]\nFCN 0.05358516\nVfB 0.11534948\nBVB 0.37609915\nHSV 0.45496621\n          [,1]\nFCN 0.05710941\nVfB 0.10143685\nBVB 0.43181251\nHSV 0.40964122\n          [,1]\nFCN 0.05474427\nVfB 0.11090813\nBVB 0.39378554\nHSV 0.44056207\n          [,1]\nFCN 0.05635438\nVfB 0.10444354\nBVB 0.41973320\nHSV 0.41946888\n         [,1]\nFCN 0.0552554\nVfB 0.1088546\nBVB 0.4020321\nHSV 0.4338579\n          [,1]\nFCN 0.05600529\nVfB 0.10584545\nBVB 0.41410704\nHSV 0.42404222\n          [,1]\nFCN 0.05549373\nVfB 0.10789820\nBVB 0.40586997\nHSV 0.43073811\n          [,1]\nFCN 0.05584269\nVfB 0.10649789\nBVB 0.41148900\nHSV 0.42617041\n          [,1]\nFCN 0.05560464\nVfB 0.10745313\nBVB 0.40765590\nHSV 0.42928633\n          [,1]\nFCN 0.05576703\nVfB 0.10680150\nBVB 0.41027071\nHSV 0.42716076\n          [,1]\nFCN 0.05565626\nVfB 0.10724602\nBVB 0.40848698\nHSV 0.42861075\n          [,1]\nFCN 0.05573182\nVfB 0.10694279\nBVB 0.40970377\nHSV 0.42762162\n          [,1]\nFCN 0.05568027\nVfB 0.10714964\nBVB 0.40887372\nHSV 0.42829637\n          [,1]\nFCN 0.05571544\nVfB 0.10700853\nBVB 0.40943995\nHSV 0.42783608\n          [,1]\nFCN 0.05569145\nVfB 0.10710479\nBVB 0.40905369\nHSV 0.42815007\n          [,1]\nFCN 0.05570781\nVfB 0.10703913\nBVB 0.40931718\nHSV 0.42793588\n          [,1]\nFCN 0.05569665\nVfB 0.10708392\nBVB 0.40913743\nHSV 0.42808199\n          [,1]\nFCN 0.05570427\nVfB 0.10705336\nBVB 0.40926005\nHSV 0.42798232\n          [,1]\nFCN 0.05569907\nVfB 0.10707421\nBVB 0.40917641\nHSV 0.42805031\n          [,1]\nFCN 0.05570262\nVfB 0.10705999\nBVB 0.40923347\nHSV 0.42800393\n         [,1]\nFCN 0.0557002\nVfB 0.1070697\nBVB 0.4091945\nHSV 0.4280356\n          [,1]\nFCN 0.05570185\nVfB 0.10706307\nBVB 0.40922110\nHSV 0.42801398\n          [,1]\nFCN 0.05570072\nVfB 0.10706759\nBVB 0.40920298\nHSV 0.42802871\n          [,1]\nFCN 0.05570149\nVfB 0.10706451\nBVB 0.40921534\nHSV 0.42801866\n          [,1]\nFCN 0.05570097\nVfB 0.10706661\nBVB 0.40920691\nHSV 0.42802552\n          [,1]\nFCN 0.05570132\nVfB 0.10706517\nBVB 0.40921266\nHSV 0.42802084\n          [,1]\nFCN 0.05570108\nVfB 0.10706615\nBVB 0.40920874\nHSV 0.42802403\n          [,1]\nFCN 0.05570125\nVfB 0.10706549\nBVB 0.40921141\nHSV 0.42802186\n          [,1]\nFCN 0.05570113\nVfB 0.10706594\nBVB 0.40920959\nHSV 0.42802334\n          [,1]\nFCN 0.05570121\nVfB 0.10706563\nBVB 0.40921083\nHSV 0.42802233\n          [,1]\nFCN 0.05570116\nVfB 0.10706584\nBVB 0.40920998\nHSV 0.42802302\n          [,1]\nFCN 0.05570119\nVfB 0.10706570\nBVB 0.40921056\nHSV 0.42802255\n          [,1]\nFCN 0.05570117\nVfB 0.10706580\nBVB 0.40921017\nHSV 0.42802287\n          [,1]\nFCN 0.05570119\nVfB 0.10706573\nBVB 0.40921044\nHSV 0.42802265\n          [,1]\nFCN 0.05570117\nVfB 0.10706577\nBVB 0.40921025\nHSV 0.42802280\n          [,1]\nFCN 0.05570118\nVfB 0.10706574\nBVB 0.40921038\nHSV 0.42802270\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921029\nHSV 0.42802277\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921035\nHSV 0.42802272\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921031\nHSV 0.42802275\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921034\nHSV 0.42802273\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921032\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706575\nBVB 0.40921033\nHSV 0.42802273\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921032\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n          [,1]\nFCN 0.05570118\nVfB 0.10706576\nBVB 0.40921033\nHSV 0.42802274\n\nlibrary(igraph)\n\nWarning: Paket 'igraph' wurde unter R Version 4.2.3 erstellt\n\n\n\nAttache Paket: 'igraph'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    decompose, spectrum\n\n\nDas folgende Objekt ist maskiert 'package:base':\n\n    union\n\ngraphObj <- graph_from_adjacency_matrix(t(W), weighted = TRUE, mode = \"directed\")\n(prVec <- page_rank(graphObj)$vector)\n\n       FCN        VfB        BVB        HSV \n0.05570118 0.10706576 0.40921033 0.42802274 \n\n# loss matrix: L_ij is number of times team i lost against team j\n#L <- t(W)\n\n\n# normalize cols to make it a column stochastic matrix\n#A <- sweep(L, 2, colSums(L), FUN = \"/\")\n\n\n\n#for (i in 1:100) {\n#    r <- d * (A %*% r) + (1 - d) * 1/p\n#    #if (sum(abs(r_new - r)) < tol) {\n#    #  break\n#    #}\n#    #r <- r_new\n#    print(r)\n#  }\n\n\n# link wo man daten als csv bekommt: https://www.hockey-reference.com/leagues/NHL_2025_games.html#games\n\nlibrary(rvest)\n\nWarning: Paket 'rvest' wurde unter R Version 4.2.3 erstellt\n\nlibrary(dplyr)\n\nWarning: Paket 'dplyr' wurde unter R Version 4.2.3 erstellt\n\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:igraph':\n\n    as_data_frame, groups, union\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# URL of the schedule page\nurl <- \"https://www.hockey-reference.com/leagues/NHL_2025_games.html\"\n\n# Read the webpage\npage <- read_html(url)\n\n# Extract the Regular Season Schedule table (it has id=\"games\")\nschedule <- page %>%\n  html_node(\"table#games\") %>%\n  html_table(header = TRUE, fill = TRUE)\n\n# Clean up data as needed\n#schedule_clean <- schedule %>%\n#  # Convert Date column to Date type\n#  mutate(Date = as.Date(Date)) %>%\n#  # Clean Time (e.g. add timezone if needed), convert numeric columns\n#  mutate(\n#    Visitor = Visitor,\n#    Home = Home,\n#    `Visitor G` = as.integer(`Visitor G`),\n#    `Home G` = as.integer(`Home G`),\n#    Att. = as.integer(gsub(\",\", \"\", Att.)),\n#    Time = Time\n#  )\n\n# View the result\n#str(schedule_clean)\n#head(schedule_clean)"
  },
  {
    "objectID": "blog/post-1/index.html",
    "href": "blog/post-1/index.html",
    "title": "First Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "blog/post-2/index.html",
    "href": "blog/post-2/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "blabla\nblubblub"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Post description for first post\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this post, I implement from scratch two different approaches for learning a ranking from data on the results of pairwise comparisons.\n\n\n\n\n\n\nMay 22, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\nPost description for second post\n\n\n\n\n\n\nMay 23, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dominik G. Eichhorn",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nso mal sehen"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Define Custom Loss Function",
    "text": "Define Custom Loss Function\n\nclass LossODE(nn.Module):\n  def __init__(self):\n    super(LossODE, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    y = model(x)\n    \n    dy_dx = torch.autograd.grad(\n      outputs=y,\n      inputs=x,\n      grad_outputs=torch.ones_like(y),\n      create_graph=True\n      )[0]\n    \n    d2y_dx2 = torch.autograd.grad(\n      outputs=dy_dx,\n      inputs=x,\n      grad_outputs=torch.ones_like(dy_dx),\n      create_graph=True)[0]\n    \n    loss_DE = torch.mean((d2y_dx2 + 1*dy_dx + 4*y)**2)\n    loss_initial = (y[0] - 0.5)**2 + (dy_dx[0] - 2)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#construct-model-loss-and-optimizer",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Construct model, loss and optimizer",
    "text": "Construct model, loss and optimizer\n\nmodel = Network(input_size=1, hidden_size=10, output_size=1)\ncriterion = LossODE()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#set-x-values",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Set x-values",
    "text": "Set x-values\n\nx = torch.linspace(0, 5, 100)[:, None]"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#training",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Training",
    "text": "Training\nWICHTIG: Kommentar auch in ANN für ODEs bzgl. epochs & PRINT REPLACE ÜBERNEHMEN\n\nseed = 42\ntorch.manual_seed(seed)\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.1986\n\n\nEpoch [2/10], Loss: 0.0790\n\n\nEpoch [3/10], Loss: 0.0292\n\n\nEpoch [4/10], Loss: 0.0050\n\n\nEpoch [5/10], Loss: 0.0018\n\n\nEpoch [6/10], Loss: 0.0010\n\n\nEpoch [7/10], Loss: 0.0008\n\n\nEpoch [8/10], Loss: 0.0006\n\n\nEpoch [9/10], Loss: 0.0004\n\n\nEpoch [10/10], Loss: 0.0002"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#visualize-learned-and-analytical-solution",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#visualize-learned-and-analytical-solution",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Visualize learned and analytical solution",
    "text": "Visualize learned and analytical solution\n\n\nShow code\nx = torch.linspace(0, 15, 100)[:, None]\nwith torch.no_grad():\n  y = model(x)\n\nplt.figure(figsize=(8, 5))\nplt.plot(x, y, label=\"Predicted\")\nplt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), '--', label=\"Analytical\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function-1",
    "href": "blog/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-custom-loss-function-1",
    "title": "Back up ODE: torch.mean( (dy_dx - (x+y)/x)2 ) + (y[0] + 0.1)2",
    "section": "Define Custom Loss Function",
    "text": "Define Custom Loss Function\n\nclass LossODE(nn.Module):\n  def __init__(self):\n    super(LossODE, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    y = model(x)\n    \n    dy_dx = torch.autograd.grad(\n      outputs=y,\n      inputs=x,\n      grad_outputs=torch.ones_like(y),\n      create_graph=True\n      )[0]\n    \n    loss_DE = torch.mean((dy_dx - 2*x*(2-y))**2)\n    loss_initial = (y[0] + 1)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total"
  }
]