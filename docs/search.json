[
  {
    "objectID": "coding/MLforAssetManagement/MLforAssetManagement.html",
    "href": "coding/MLforAssetManagement/MLforAssetManagement.html",
    "title": "Machine Learning for Managing Stock Portfolios",
    "section": "",
    "text": "Before Diving In\nThose interested in the technical details may scroll down to the bottom (section Appendix), where the pdf-file of my master’s thesis is displayed. Unlike the other posts, in the remainder of this post, I will not go into the technical details or show any code. I will rather give an high-level overview of what I did as part of my master’s thesis.\n\n\nIntroduction\nIn their seminal paper, Gu, Kelly, and Xiu (2020) describe how supervised machine learning models can be utilized to manage stock portfolios. They report that empirically these so-called machine learning portfolios deliver substantial risk-adjusted outperformance. However, their analyses only consider gross returns, i.e., before transaction costs. This is a problem insofar as these machine learning portfolios very frequently buy stocks and sell them shortly afterwards (or the other way around). Each of these stock-level transactions leads to costs which reduce the net returns of the portfolio. It is therefore a fair question what the net return of these machine learning portfolios is.\nIn trying to answer this question, I encountered two major challenges. First, Gu, Kelly, and Xiu (2020) do not provide any code for the construction of the machine learning portfolios. Hence, I had to replicate their approach from scratch in Python based on the descriptions in their paper. Second, in order to calculate the net returns I need at every point in time the stock-level transaction costs for all stocks considered (or at least an estimate of them). My supervisor pointed out to me that such a data set had recently been made publicly available by Chen and Zimmermann (2021), which I then used for my analyses.\nIn the following, I will first briefly say something about the data set I used. Then, I will give a high-level description of how I constructed the machine learning portfolios. Subsequently, I will present my results on their after-transaction-cost performance.\n\n\nData\nThe data, upon which my analyses were based, covered monthly information on approx. 8,000 major US exchanged-listed stocks between 1970 and 2017. For every stock, I have for each month a set of 202 stock specific characteristics, its gross return as well as its transaction/trading cost (i.e. half the spread).\n\n\nApproach\nThe foundation upon which machine learning portfolios are based is a prediction model. The prediction model takes the characteristics of each stock in month \\(t\\) as input and outputs for each stock a prediction of the gross return realized in month \\(t+1\\). The prediction model is gradually updated overtime using an expanding window approach.\nI trained and tuned a broad variety of prediction models. These include several baseline unpenalized and penalized linear models, tree based models such as random forests and gradient boosted trees, and multiple deep learning models. Note that each prediction model is the foundation for a different machine learning portfolio.\nGiven a prediction model, at the end of month \\(t\\), the machine learning portfolio is constructed as follows: The prediction model is used to predict the gross return of each stock in month \\(t+1\\). Next, all stocks are sorted according to their predicted gross return for month \\(t+1\\). Subsequently, the portfolio is formed by going long in the top 10% and going short the bottom 10% of stocks sorted by predicted return. The remaining 80% of the stocks are kept neutral (zero position).\nOver time, the predicted returns for a given stock tend to fluctuate a lot. Hence, a stock that ranks in the top 10% by predicted return in one month seldom remains there in the subsequent month (same holds for bottom 10%). As a consequence, at the end of each month many transactions need to be executed in order to close existing long and short positions and open new ones. Each of these transactions causes costs which in turn shrink net returns. The data set by provided by Chen and Zimmermann (2021) containing monthly stock-level information on the transaction costs allows me to exactly compute those net returns of the machine learning portfolios I constructed.\n\n\nResults\nThe tables and plots shown below are all taken from my master’s thesis (provided in the Appendix at the bottom of this page).\nAs mentioned above, each prediction model is the foundation for a different machine learning portfolio. The different machine learning portfolios are respectively based on: unpenalized linear models (\\(OLS-3\\), \\(OLS-all\\)), penalized linear models (\\(Ridge\\), \\(LASSO\\), \\(E-Net\\), i.e., elastic net), tree-based models (\\(RF\\), i.e. random forest, \\(XGBoost\\), i.e. boosted trees), and deep neural networks with 1 to 5 hidden layers (\\(NN1\\), \\(NN2\\), \\(NN3\\), \\(NN4\\), \\(NN5\\)) and an ensemble of them (\\(NN\\text{1-5}\\)).\n\nThe above plot, depicts the cumulative out-of sample net performance of all the machine learning portfolios I constructed. The below table reports among others the average out-of sample monthly net returns (\\(Avg(n)\\) column) and the corresponding values of the t-statistic (\\(t(n)\\) column). Taken together, two insights can be drawn: Firstly, all considered machine learning portfolios earn statistically significant positive average monthly net returns. Secondly, the machine learning portfolios that are based on deep neural networks yield net returns that are substantially larger than those based on the other models.\n\nHowever, it might by that all the considered machine learning portfolios earn rather high net returns just because they excessively take risks and are therefore rewarded with higher returns. Thus, I had a look at a risk adjusted measure of (out-) performance, namely the alpha against the Fama-French 5 Factor Model plus Momentum (FF5M+Mom). The below table reports among others for all machine learning portfolios the alphas after transaction costs (i.e. net alphas in \\(\\alpha(n)\\) column) and the corresponding values of the t-statistic (in parentheses below). From this it can be seen that only the machine learning portfolios based on random forests and on deep neural networks yield statistically significant risk-adjusted outperformance (against the FF5M+Mom) after transaction costs.\n\n\n\nAppendix\n\n\n\n\n\n\nReferences\n\nChen, Andrew Y, and Tom Zimmermann. 2021. “Open Source Cross-Sectional Asset Pricing.” Critical Finance Review, Forthcoming.\n\n\nGu, Shihao, Bryan Kelly, and Dacheng Xiu. 2020. “Empirical Asset Pricing via Machine Learning.” The Review of Financial Studies 33 (5): 2223–73."
  },
  {
    "objectID": "coding/LearningToRank/LearningToRank.html",
    "href": "coding/LearningToRank/LearningToRank.html",
    "title": "Learning to Rank",
    "section": "",
    "text": "Probably everyone is familiar with those tables in sports that rank teams, usually according to the number of games won. This seems to be a valide approach for tournaments where every team plays against every other team the same number of times. Such tournaments are called robin-round tournaments and examples include the Premier League as well as the Fußball and the Handball Bundesliga. Robin-round tournaments are a special type of complete comparisons.\nHowever, there are also tournaments in which not every team plays against every other team, i.e. incomplete comparisons. Examples include the NFL in American Football or knock-out tournaments such as playoffs or cups. In such tournaments, some teams might face on average stronger opponents than other teams. Therefore, the number of wins may not reflect the true strength of a team and may not be used to rank teams. Instead, it becomes important, how strong the opponents were, against which a team won or lost.\nSo I was wondering, what approaches there are for learning a ranking from data on the outcome of incomplete pairwise comparisons. Note that this problem does not just arise in sports but in a variety of other domains. For instance, in preference modelling, multiple individuals must choose between different pairs of items. Based on the outcomes, one tries to rank all of the items. Nevertheless, I will stick to the application in sports.\nI found among others two interesting approaches: the PageRank algorithm and the Bradley-Terry model. I will implement both from scratch in R after having shortly described them. Furthermore, I will compare the results of my own implementation to the ones returned by dedicated libraries. For illustration, I will use data on the outcome of every NFL game played during the 2024-2025 regular season."
  },
  {
    "objectID": "coding/LearningToRank/LearningToRank.html#approach",
    "href": "coding/LearningToRank/LearningToRank.html#approach",
    "title": "Learning to Rank",
    "section": "Approach",
    "text": "Approach\nUnder the Bradley-Terry model, the probability that in a direct comparison team \\(i\\) would beat team \\(j\\) is given by: \\[ P(\\text{\"i beats j\"}) = \\frac{\\theta_i}{\\theta_i + \\theta_j}\\] for \\(i,j = 1, \\dots, n, i \\neq j\\). \\(\\theta_i\\) can be thought of as measuring the strength of team \\(i\\). Ceteris paribus, the greater \\(\\theta_i\\), the greater the probability that team \\(i\\) wins against any other team.\nUnder independence, the likelihood function (i.e. the probability of the observed data under \\(\\theta_1, \\dots, \\theta_n\\)) is \\[L(\\theta_1, \\dots, \\theta_n) =  \\prod_{i=1}^n \\prod_{j=1}^n \\left(\\frac{\\theta_i}{\\theta_i + \\theta_j}\\right)^{w_{ij}}.\n\\] Note that \\(w_{ii}\\) is set to 0 as mentioned above. The log-likelihood function is \\[ l(\\theta_1, \\dots, \\theta_n) = \\sum_{i=1}^n \\sum_{j=1}^n w_{ij} \\ln(\\theta_i) - \\sum_{i=1}^n \\sum_{j=1}^n w_{ij} \\ln(\\theta_i + \\theta_j)\\] The maximum likelihood estimates (MLEs) are obtained by maximizing \\(l\\) w.r.t. \\(\\theta_1, \\dots, \\theta_n\\). For this purpose the gradient of \\(l\\) is required. It is given by: \\[ \\begin{align} \\frac{\\partial l}{\\partial \\theta_k}(\\theta_1, \\dots, \\theta_n) &= \\frac{1}{\\theta_k} \\sum_{j=1}^n w_{kj} - \\left[ \\overbrace{ \\sum_{i=1}^n \\frac{w_{ik}}{\\theta_i + \\theta_k}}^{\\text{case: }j=k} + \\overbrace{\\sum_{j=1}^n \\frac{w_{kj}}{\\theta_k + \\theta_j}}^{\\text{case: }i=k} \\right], \\\\\n&= \\frac{1}{\\theta_k} \\sum_{j=1}^n w_{kj} - \\left[ \\sum_{j=1}^n \\frac{w_{jk}}{\\theta_j + \\theta_k} + \\sum_{j=1}^n \\frac{w_{kj}}{\\theta_k + \\theta_j} \\right],\\\\\n&= \\frac{1}{\\theta_k} \\sum_{j=1}^n w_{kj} - \\sum_{j=1}^n \\frac{w_{jk} +w_{kj}}{\\theta_j + \\theta_k} ,  &\\forall k=1,\\dots n \\end{align}\\] Note that I index the differentiation variable by \\(k\\) in order to clearly separate it from the summation index \\(i\\). Setting the gradient equal to 0 and rearranging yields: \\[ \\theta_k = \\frac{\\sum_{j=1}^n w_{kj}}{\\sum_{j=1}^n \\frac{w_{kj} + w_{jk}}{\\theta_k + \\theta_j}}, \\; \\forall k=1,\\dots,n \\] Starting with some initial values \\(\\theta_1^{(0)}, \\dots, \\theta_k^{(0)}\\), and using the above equation, the MLEs are iteratively obtained as follows: \\[\\begin{align}\n\\theta_k^{(t+1)} &\\gets \\frac{\\sum_{j=1}^n w_{kj}}{\\sum_{j=1}^n \\frac{w_{kj} + w_{jk}}{\\theta_k^{(t)} + \\theta_j^{(t)}}}, &\\; \\forall k=1,\\dots,n \\\\\n\\theta_k^{(t+1)} &\\gets \\frac{1}{\\left(\\prod_{j=1}^n \\theta_j^{(t+1)} \\right)^{1/n}}, &\\; \\forall k=1,\\dots,n\n\\end{align}\\] The second operation normalizes the MLEs such that their geometric mean is 1 (Newman 2023)."
  },
  {
    "objectID": "coding/LearningToRank/LearningToRank.html#implementation",
    "href": "coding/LearningToRank/LearningToRank.html#implementation",
    "title": "Learning to Rank",
    "section": "Implementation",
    "text": "Implementation\nHaving outlined how the MLEs are obtained in the Bradley-Terry model, below, you find my from-scratch implementation of it.\n\nmy_bradleyterry &lt;- function(W, n_iter=1000){\n  n &lt;- dim(W)[1]\n  \n  # initialize strength vector theta,\n  # before and after update,\n  # with all teams equally strong\n  theta.old &lt;- rep(1, n)\n  names(theta.old) &lt;- colnames(W)\n  theta.new &lt;- rep(1,n)\n  names(theta.new) &lt;- colnames(W)\n  \n  for (t in 1:n_iter) {\n    for (i in 1:n) {\n      theta.new[i]&lt;-sum(W[i,])/(sum((W[i,]+W[,i])/(theta.old[i]+theta.old )))\n    }\n    \n    # normalize by geomtric mean\n    theta.new &lt;- theta.new/prod(theta.new)^(1/n)\n    \n    # update\n    theta.old &lt;- theta.new\n  }\n  \n  return(theta.new)\n}\n\nI apply my function for computing the MLEs to the winning matrix of the 2024-2025 regular season retrieved above.\n\nmy_btm_mles &lt;- my_bradleyterry(W)\nhead(my_btm_mles)\n\nArizona Cardinals   Atlanta Falcons  Baltimore Ravens     Buffalo Bills \n        1.0972960         0.9883723         3.4372074         3.5989702 \nCarolina Panthers     Chicago Bears \n        0.3647306         0.4153262 \n\n\nNext, I sort the teams in descending order according to their estimated strengths and construct a horizontal bar char.\n\n\nShow code\nlibrary(ggplot2)      # for visualization\n\ndf &lt;- data.frame(\n  team = names(my_btm_mles),\n  score = as.numeric(my_btm_mles)\n)\n\np &lt;- ggplot(df, aes(reorder(team, score), y=score)) +\n  geom_col(fill=\"#0b0b64\") +\n  coord_flip() +\n  labs(title=\"Ranking and Strength based on Bradley-Terry\",\n       y=\"Strength\",\n       x=\"Team\") +\n  theme_light()\n\nggsave(\"cover.jpg\", plot = p, width = 8, height = 6, dpi = 300, bg = \"white\")\n\np\n\n\n\n\n\n\n\n\n\nNow, let’s compare the results of my from-scratch implementation to those of the BradleyTerry2 R package.\n\n# use built-in function to convert W in format required by BradleyTerry2\ndt &lt;- BradleyTerry2::countsToBinomial(W)\n\n# fit Bradley-Terry model using BradleyTerry2\npackage_btm &lt;- BradleyTerry2::BTm(cbind(win1, win2), player1, player2, data=dt)\n\n# extract Bradley-Terry model MLEs\npackage_btm_mles &lt;- BradleyTerry2::BTabilities(package_btm)\n\n\n# compare results\nbtm_compare &lt;- cbind(my_btm_mles, \"package_btm_mles\"=package_btm_mles[,\"ability\"])\nhead(btm_compare)\n\n                  my_btm_mles package_btm_mles\nArizona Cardinals   1.0972960        0.0000000\nAtlanta Falcons     0.9883723       -0.1045449\nBaltimore Ravens    3.4372074        1.1418104\nBuffalo Bills       3.5989702        1.1877988\nCarolina Panthers   0.3647306       -1.1014454\nChicago Bears       0.4153262       -0.9715400\n\n\nAt first sight, it seems that the results returned by the BradleyTerry2 package and my results differ quite alot. Therefore, I had a closer look at the documentation of the BradleyTerry2 package. I found out that they utilize a slightly different specification of the Bradley-Terry model (let’s call it the exponential specification). However, both specifications are equivalent in the sense that, first, both result in the same winning probabilities and, second, that there is an explicit formula for converting their parameters.\nThe exponential specification is given by: \\[P(\\text{\"i beats j\"}) = \\frac{e^{s_i}}{e^{s_i} + e^{s_j}}, \\; \\forall i,j=1,\\dots, n\\] and \\(s_0\\) is set to be 0. The formulas for converting the parameters of the two specifications are: \\[ \\begin{align} s_i = \\ln \\left(\\frac{\\theta_i}{\\theta_0}\\right), \\\\\n\\theta_i = \\exp(s_i) \\times \\theta_0\n\\end{align}\\] So, let’s now convert the parameters I estimated into the format of the exponential specification and then compare it to the output of the BradleyTerry2 package.\n\n\n# compare results\nmy_conv_btm_mles &lt;- log(my_btm_mles/my_btm_mles[1])\nbtm_compare &lt;- cbind(my_btm_mles, my_conv_btm_mles, \"package_btm_mles\"=package_btm_mles[,\"ability\"])\n\n\n# compute Euclidean distance between my_conv_btm_mles and package_btm_mles\ndist &lt;- sqrt(sum((btm_compare[,2]-btm_compare[,3])^2))\nprint(paste(\"Euclidean distance between my_conv_btm_mles and package_btm_mles:\", dist))\n\n[1] \"Euclidean distance between my_conv_btm_mles and package_btm_mles: 5.39456400516441e-12\"\n\nhead(btm_compare)\n\n                  my_btm_mles my_conv_btm_mles package_btm_mles\nArizona Cardinals   1.0972960        0.0000000        0.0000000\nAtlanta Falcons     0.9883723       -0.1045449       -0.1045449\nBaltimore Ravens    3.4372074        1.1418104        1.1418104\nBuffalo Bills       3.5989702        1.1877988        1.1877988\nCarolina Panthers   0.3647306       -1.1014454       -1.1014454\nChicago Bears       0.4153262       -0.9715400       -0.9715400\n\n\nAfter converting my MLEs, they are exactly the same as the MLEs returned by the BradleyTerry2 package."
  },
  {
    "objectID": "coding/LearningToRank/LearningToRank.html#approach-1",
    "href": "coding/LearningToRank/LearningToRank.html#approach-1",
    "title": "Learning to Rank",
    "section": "Approach",
    "text": "Approach\nPageRank does not treat all of the opponents of a given team equally. In fact, it accounts for both, the strength of the opponents as well as their total number of games lost. Opponents with higher strength get more weight, while opponents with a higher total number of games lost get a less weight.\nBased on this mechanism, the strength of team \\(i\\) according to PageRank, \\(p_i\\), is recursively defined as: \\[ p_i = (1-d) + d \\sum_{j=1}^n \\left( \\frac{w_{ij}}{c_j} \\right) p_j \\] As before, \\(w_{ij}\\) is the number of times team \\(i\\) won against team \\(j\\). \\(c_j = \\sum_{i=1}^n w_{ij}\\) is the total numer of games lost by team \\(j\\). The term inside the sum works according to the above described mechanism: Wins against an opponent \\(j\\) with high strength (i.e. high \\(p_j\\)) get more weight, while wins against an opponent with a high number of total games lost (i.e. high \\(c_j\\)) get less weight. This is then summed up across all opponents and weighted by the damping factor \\(d \\in [0,1)\\) (usually, \\(d\\) is set to 0.85). \\(d\\) guarantees the existence of (finite) soultions even if there is a team that always won or always lost.\nThe above can be written in matrix-vector notation as follows: \\[ \\textbf{p} = (1-d) \\, \\textbf{e} + d \\, \\textbf{W} \\,\\textbf{D}_c^{-1} \\, \\textbf{p} \\] where \\(\\textbf{p}=[p_1, \\dots, p_n]^T\\) is the PageRank strength vector and \\(\\textbf{e}\\) is an \\(n\\)-dimensional vector of ones. \\(\\textbf{D}=\\text{diag}(c_1, \\dots, c_n)\\) is a diagonal matrix and \\(\\textbf{W}\\) is the win matrix introduced above.\nTo get a unique solution, PageRank requires a restriction on the scale of the strength vector. Therefore, the restriction that \\(\\textbf{p}\\) must to sum up to \\(n\\), i.e. \\(\\textbf{e}^T \\, \\textbf{p} = n\\), is introduced. This is the same as saying that the average strength is 1. Using this, the following holds: \\[ \\textbf{e} = \\frac{\\textbf{e} \\, \\textbf{e}^T \\, \\textbf{p}}{n}\\] Plugging this into the recursive formula for \\(\\textbf{p}\\) and factoring out \\(\\textbf{p}\\) on the right hand side yields: \\[ \\begin{align}\n\\textbf{p} &= \\left[ (1-d) \\, \\textbf{e} \\, \\textbf{e}^T/n + d \\, \\textbf{W} \\,\\textbf{D}_c^{-1} \\right] \\textbf{p} \\\\\n&= \\mathbf{A} \\, \\mathbf{p}\n\\end{align}\\]\nSo the PageRank strength vector is the eigenvector corresponding to an eigenvalue of 1 of matrix \\(\\textbf{A}\\). In fact, it can be shown that 1 is the largest eigenvalue of \\(\\textbf{A}\\). It is known that the power iteration (or Von Mises iteration) method converges to the eigenvector corresponding to the largest eigenvalue, in this case the eigenvector corresponding to the eigenvalue 1. So given some initial vector\\(\\textbf{p}_0\\), the PageRank strength vector can be calculated iteratively as: \\[\n\\begin{align}\n\\textbf{p}^{(t+1)} &\\gets \\textbf{A} \\, \\textbf{p}^{(t)} \\\\\n\\textbf{p}^{(t+1)} &\\gets \\frac{\\textbf{p}^{(t+1)}}{\\textbf{e}^T \\, \\textbf{p}^{(t+1)}} n\n\\end{align}\n\\] The second operation is a normalization ensuring that \\(\\textbf{e}^T \\, \\textbf{p}^{(t+1)} = n\\) Hastie et al. (2009)."
  },
  {
    "objectID": "coding/LearningToRank/LearningToRank.html#implementation-1",
    "href": "coding/LearningToRank/LearningToRank.html#implementation-1",
    "title": "Learning to Rank",
    "section": "Implementation",
    "text": "Implementation\nBelow, you can see my implementation of the PageRank approach.\n\nmy_pagerank &lt;- function(W, d=0.85, n_iter=1000){\n  n &lt;- dim(W)[1]\n  c &lt;- colSums(W)\n  A &lt;- (1-d)*rep(1,n) %*% t(rep(1,n))/n + d*W%*%diag(1/c)\n  \n  # initialize pagerank strength vector\n  # with all teams equally strong\n  p &lt;- rep(1/n, n)\n  names(p) &lt;- colnames(W)\n  \n  # power iteration\n  for (t in 1:n_iter) {\n    p &lt;- A%*%p\n    p &lt;- (p/sum(p))*n\n  }\n  \n  return(p)\n}\n\nLet’s apply my implementation of the PageRank to the win matrix.\n\nmy_pagerank_est &lt;- my_pagerank(W)\nhead(my_pagerank_est)\n\n                       [,1]\nArizona Cardinals 0.7385196\nAtlanta Falcons   1.2251638\nBaltimore Ravens  1.9602687\nBuffalo Bills     2.2841688\nCarolina Panthers 0.4254393\nChicago Bears     0.5206048\n\n\nAnd again I visualize the estimated strengths and the ranking using a horizontal bar char.\n\n\nShow code\ndf &lt;- data.frame(\n  team = rownames(my_pagerank_est),\n  score = as.numeric(my_pagerank_est)\n)\n\nggplot(df, aes(reorder(team, score), y=score)) +\n  geom_col(fill=\"#d72638\") +\n  coord_flip() +\n  labs(title=\"Ranking and Strength based on PageRank\",\n       y=\"Strength\",\n       x=\"Team\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nAs for the Bradley-Terry model, I want to compare my from-scratch implementation of PageRank to the one provided by the igraph R package. Before actually compute the PageRank strength, the igraph package requires to construct a graph using the transpose of the win matrix as adjacency matrix.\n\n# construct graph from tranpose of win matrix\ngraphObj &lt;- igraph::graph_from_adjacency_matrix(t(W), weighted = TRUE, mode = \"directed\")\n\n# run pagerank\npackage_pagerank_est &lt;- igraph::page_rank(graphObj)$vector\n\n# compare results\npagerank_compare &lt;- cbind(my_pagerank_est,\n                          package_pagerank_est,\n                          my_pagerank_est/package_pagerank_est)\ncolnames(pagerank_compare) &lt;- c(\"my_pagerank_est\",\"package_pagerank_est\",\"ratio\")\nhead(pagerank_compare)\n\n                  my_pagerank_est package_pagerank_est ratio\nArizona Cardinals       0.7385196           0.02307874    32\nAtlanta Falcons         1.2251638           0.03828637    32\nBaltimore Ravens        1.9602687           0.06125840    32\nBuffalo Bills           2.2841688           0.07138027    32\nCarolina Panthers       0.4254393           0.01329498    32\nChicago Bears           0.5206048           0.01626890    32\n\n\nObviously, my values and the values returned by the igraph package are not the same. However, when dividing them by each other it becomes apparent that my values are just 32 times the values returned by the package. The reason for the difference in scale is the following: The igraph package uses as restriction on the strength vector that it sums up to 1. However, I use as restriction that the strength vector sums up to \\(n=32\\). Importantly, this difference in scaling has no influence on the rankings."
  },
  {
    "objectID": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "href": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "",
    "text": "The impressive capabilitis of neural networks in typical regression and classification tasks are widely known. Recently, I came across a paper about a lesser known but statistically very interesting application of neural networks that was written some years ago. The paper shows how neural networks can be used for estimating the probability density function of a random variable (Magdon-Ismail and Atiya 1998). Before implementing it from scratch in Python, I will first describe the general approach."
  },
  {
    "objectID": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#define-neural-network",
    "href": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#define-neural-network",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "Define Neural Network",
    "text": "Define Neural Network\nAt first, I define the neural network used for approximating the CDF. It is a fully connected feed forward neural network with one hidden layer using a \\(tanh\\) activation function and an output layer using a \\(sigmoid\\) activation function. Thereby, the neural network is differentiable and maps \\(\\mathbb{R}\\) onto the interval \\((0,1)\\).\n\nclass N(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(N, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Tanh(),\n      nn.Linear(hidden_size, output_size),\n      nn.Sigmoid()\n      )\n    \n  def forward(self, x):\n    return self.net(x)"
  },
  {
    "objectID": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#define-modified-loss-function",
    "href": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#define-modified-loss-function",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "Define Modified Loss Function",
    "text": "Define Modified Loss Function\nNext, I define the modified loss function as described above.\n\nclass Loss(nn.Module):\n  def __init__(self):\n    super(Loss, self).__init__()\n  \n  def forward(self, y_pred, y_true, lambda_mon, mon_l, mon_u):\n    loss_dist = torch.mean((y_pred - y_true)**2)\n    \n    diff = mon_l - mon_u\n    loss_mon = torch.mean(torch.clamp(diff, min=0))\n    \n    loss_total = loss_dist + lambda_mon*loss_mon\n    \n    return loss_total"
  },
  {
    "objectID": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#gaussian-mixture",
    "href": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#gaussian-mixture",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "Gaussian Mixture",
    "text": "Gaussian Mixture\nFor the first example, realizations will be drawn from a 2-component Gaussian mixture distribution. So I construct one with mixture weights \\(\\phi_1=0.2\\) and \\(\\phi_2=0.8\\), expectations \\(\\mu_1=2\\) and \\(\\mu_2=5\\) and standard deviations \\(\\sigma_1=0.25\\) and \\(\\sigma_2=0.5\\).\n\n# construct 2-component Gaussian mixture distribution\ncomp1 = stats.Normal(mu=2, sigma=0.25)\ncomp2 = stats.Normal(mu=5, sigma=0.5)\nmix = stats.Mixture([comp1, comp2], weights=[0.2, 0.8])\n\nThe corresponding PDF looks as follows:\n\n\nShow code\nx = np.linspace(0, 10, 1000)\n\nplt.figure(figsize=(6,5))\nplt.plot(x, mix.pdf(x), color=\"C0\", linestyle=\"dotted\")\nplt.title(\"True Probability Density Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSet Seed\nIn the subsequent steps, the neural network will be initialized and realizations will be sampled from the PDF given above. Both involves randomness. Therefore, to ensure reproducibility of the subsequent steps, one must set a seed for the random number generators.\n\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n&lt;torch._C.Generator at 0x1cfb761ead0&gt;\n\n\n\n\nConstruct Model, Loss and Optimizer\nAfter that, I instantiate the neural network, the loss function to be minimized and the optimizer (here, I use Adam).\n\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSample from PDF\nNext, I sample \\(n=200\\) i.i.d. realizations from the 2-component Gaussian mixture distribution. Subsequently, I sort then in ascending order.\n\n# sample data, sort it and convert to torch tensor\nn = 200\nrng = np.random.default_rng(seed)\nx = mix.sample(n, rng=rng)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n\n\nSet Monotonicity Points\nBefore training can start, I have to specify \\(\\lambda_{mon}\\), \\(n_{mon}\\), the monotonicity points themselves as well as \\(\\Delta\\). I choose to place \\(n_{mon}=1000\\) monotonicity points equally spaced between the smallest and the largest sampled \\(X\\)-values.\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon\n\n\n\nTraining\nNow, everything is set up and training can start. Observe that, as described in the general approach, in each iteration, a new set of \\(n\\) samples are drawn i.i.d. from \\(\\mathcal{U}(0,1)\\) and sorted in ascending order. These serve as targets only for that iteration.\n\nnum_epochs = 100\nsteps_per_epoch = 2500\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\", end=\"\\r\", flush=True)\n\n\n\nPlot CDF\nOnce training has finished, I plot the learned neural network approximation of CDF against the actual CDF of the 2-component Gaussian mixture.\n\n\nShow code\nxx = torch.linspace(0, 10, 1000).unsqueeze(1)\nwith torch.no_grad():\n    cdf_est = model(xx)\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx.numpy(), cdf_est.numpy(), color=\"C3\", label=\"Estimated CDF\")\nplt.plot(xx.numpy(), mix.cdf(xx), color=\"C0\", label=\"True CDF\", linestyle=\"dotted\")\nplt.title(\"Cumulative Density Function (CDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Cumulative Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nObviously, the neural network approximation of the CDF given just a sample of 200 realizations of \\(X\\) is very accurate.\n\n\nCompute PDF\nGiven the neural network approximation of the CDF, the following function computes an estimate of the PDF from it.\n\ndef ComputePDF(model, x):\n  x = x.requires_grad_(True)\n  cdf = model(x)\n  grad_outputs = torch.ones_like(cdf)\n  pdf = torch.autograd.grad(\n    outputs=cdf,\n    inputs=x,\n    grad_outputs=grad_outputs\n    )[0]\n  return pdf\n\n\n\nPlot PDF\nThis plot confirms, that the estimate of the PDF obtained from the neural network approximation to the CDF is quite precise.\n\n\nShow code\npdf_est = ComputePDF(model, xx).detach()\nxx_np = xx.detach().numpy()\n\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx_np, pdf_est.numpy(), color=\"C3\", label=\"Estimated PDF\")\nplt.plot(xx_np, mix.pdf(xx_np), color=\"C0\", label=\"True PDF\", linestyle=\"dotted\")\nplt.title(\"Probability Density Function (PDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#weibull-distribution",
    "href": "coding/ANNsForProbabilityDensityEstimation/ANNsForProbabilityDensityEstimation.html#weibull-distribution",
    "title": "Neural Networks for Probability Density Estimation",
    "section": "Weibull Distribution",
    "text": "Weibull Distribution\nAs second example, I will use a Weibull distribution with \\(shape=1.8\\) (and \\(scale=1\\)).\n\n# construct Weibull distribution\ndist = stats.weibull_min(c=1.8)\n\nA plot of its PDF shows that it is clearly skewed.\n\n\nShow code\nx = np.linspace(0, 5, 1000)\n\nplt.figure(figsize=(6,5))\nplt.plot(x, dist.pdf(x), color=\"C0\", linestyle=\"dotted\")\nplt.title(\"True Probability Density Function\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe following steps (set seed; construct model, loss and optimizer; sample from PDF; set monontonicity points; training) are analogous to those in the first example. Therefore, I hide that part of the code. If you wish to have a look at it, click on the “Show code” button below.\n\n\nShow code\n# set seed\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# construct model, loss and optimizer\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n# sample data, sort it and convert to torch tensor\nn = 200\nrng = np.random.default_rng(seed)\nx = dist.rvs(n, random_state=rng)\nx = torch.from_numpy(np.sort(x)).float().unsqueeze(1)\n\n# set monotonicity points\nlambda_mon = 1e6\nn_mon = 1000\nmon_points = torch.linspace(x[0,0], x[-1,0], n_mon)[:, None]\ndelta = 0.1*(max(x)-min(x))/n_mon\n\n\nnum_epochs = 100\nsteps_per_epoch = 2500\n\n# training\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    preds = model(x)\n    \n    u = np.random.uniform(0, 1, n)\n    u = torch.from_numpy(np.sort(u)).float().unsqueeze(1)\n    \n    mon_l = model(mon_points)\n    mon_u = model(mon_points + delta)\n    \n    loss = criterion(preds, u, lambda_mon, mon_l, mon_u)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\", end=\"\\r\", flush=True)\n\n\n\nPlot CDF\nInspecting the below plot of the learned neural network approximation of the CDF and the actual CDF shows again that the neural network was successful in recovering the actual CDF.\n\n\nShow code\nxx = torch.linspace(0, 5, 1000).unsqueeze(1)\nwith torch.no_grad():\n    cdf_est = model(xx)\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx.numpy(), cdf_est.numpy(), color=\"C3\", label=\"Estimated CDF\")\nplt.plot(xx.numpy(), dist.cdf(xx), color=\"C0\", label=\"True CDF\", linestyle=\"dotted\")\nplt.title(\"Cumulative Density Function (CDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Cumulative Density\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPlot PDF\nAlso when comparing the resulting estimated PDF to the actual one, it seems that neural network approach yields good results.\n\n\nShow code\npdf_est = ComputePDF(model, xx).detach()\nxx_np = xx.detach().numpy()\n\n\nplt.figure(figsize=(6, 5))\nplt.plot(xx_np, pdf_est.numpy(), color=\"C3\", label=\"Estimated PDF\")\nplt.plot(xx_np, dist.pdf(xx_np), color=\"C0\", label=\"True PDF\", linestyle=\"dotted\")\nplt.title(\"Probability Density Function (PDF)\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "My name is Dominik and I am a statistician. I love working with data, solving complex problems, and turning insights into impact.\nMy journey has taken me from studying maths and business at TUM to diving deep into statistics and machine learning. Currently, I am a Ph.D. candidate at the Chair of Statistics at the University of the Bundeswehr Munich. I have already had the opportunity to gather some practical experience through internships (e.g. at Allianz) and project work (PwC in cooperation with TUM Data Innovation Lab).\nFor me, work is most fun when you develop together data-driven solutions and put them into practice. I love expanding my skills continuously, e.g., by taking online courses or reading scientific literature on topics that are new to me.\nIn my leisure time, I enjoy rowing in a crew, cooking, travelling and reading."
  },
  {
    "objectID": "coding/index.html",
    "href": "coding/index.html",
    "title": "Projects",
    "section": "",
    "text": "Neural Networks for Solving Ordinary Differential Equations\n\n\nI code from scratch a neural network based approach for numerically solving ordinary differential equations (ODE). I illustrate how this method functions by applying it to two examplary ODEs.\n\n\n\n\n\nOct 7, 2025\n\n\nDominik G. Eichhorn\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Networks for Probability Density Estimation\n\n\nI implement from scratch an approach for estimating an unknown probability density function of a sample using neural networks. To demonstrate that this method works, I apply it to two examples.\n\n\n\n\n\nSep 10, 2025\n\n\nDominik G. Eichhorn\n\n\n\n\n\n\n\n\n\n\n\n\nLearning to Rank\n\n\nI implement from scratch two approaches for learning a ranking from imcomplete pairwise comparison data (Bradley-Terry model and PageRank). Then, I apply them to NFL data and compare the results.\n\n\n\n\n\nAug 21, 2025\n\n\nDominik G. Eichhorn\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Library Usage in England\n\n\nI do a short analysis (incl. visualizations) of library usage across districts in England. More specifically, I look at differences accross district types and at spatial dependencies.\n\n\n\n\n\nJul 29, 2025\n\n\nDominik G. Eichhorn\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning for Managing Stock Portfolios\n\n\nAs part of my master’s thesis I implemented a machine learning based approach for managing stock portfolios. I describe how this approach works and assess the performance of these portfolios.\n\n\n\n\n\nMay 1, 2022\n\n\nDominik G. Eichhorn\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coding/LibrariesInUK/LibrariesInUK.html",
    "href": "coding/LibrariesInUK/LibrariesInUK.html",
    "title": "Analyzing Library Usage in England",
    "section": "",
    "text": "Introduction\nI really enjoy going to libraries, be it to research something academic or to read for pleasur. My passion for visiting libraries prompted me to check whether there are any data sets on library usage. Unfortunately, I couldn’t find such a data set for Germany. However, I came accross some data on the usage and access to libraries in England. In the following, I will provide a short analysis (incl. visualizations) of that data.\n\n\nData\nMy analyses make use of information contained in two data sets provided by the UK Office of National Statistics:\n\nThe first, reports for every Local Authority District (LAD) in England the percentage of residents that visited a library in 2024. I will refer to this data set as the usage data set. The data is provided under the following url: https://www.ons.gov.uk/explore-local-statistics/indicators/visited-a-public-library.\n\n\n# read in data set & name cols\nusage_df &lt;- read.csv(\"visited-a-public-library-table-data.csv\", skip = 7)\ncolnames(usage_df) &lt;- c(\"LAD_code\", \"LAD_name\", \"visits\")\n\nhead(usage_df)\n\n   LAD_code             LAD_name visits\n1 E06000001           Hartlepool     16\n2 E06000002        Middlesbrough     20\n3 E06000003 Redcar and Cleveland     19\n4 E06000004     Stockton-on-Tees     21\n5 E06000005           Darlington     17\n6 E06000006               Halton     24\n\n\n\nThe second, reports for every LAD in England the percentage of residents living within a 30 minute walk of the nearest library as of 2024. This is the same as the percentage of residents living within a radius of 3km of the nearest library (given that the average walking speed is 6 kmh). I will refer to this data set as the access data set. The data is provided under the following url: https://www.ons.gov.uk/explore-local-statistics/indicators/library-walk.\n\n\n# read in data set & name cols\naccess_df &lt;- read.csv(\"library-walk-table-data.csv\", skip = 7)\ncolnames(access_df) &lt;- c(\"LAD_code\", \"LAD_name\", \"proximity\")\n\nhead(access_df)\n\n   LAD_code             LAD_name proximity\n1 E06000001           Hartlepool        92\n2 E06000002        Middlesbrough        96\n3 E06000003 Redcar and Cleveland        86\n4 E06000004     Stockton-on-Tees        88\n5 E06000005           Darlington        57\n6 E06000006               Halton        62\n\n\n\n\nLibrary Usage by District Type\nI was wondering whether the LAD-codes only serve as a unique identifier or whether they provide further information on the respective LAD. After some research on the internet, I found out that in fact the first three characters of the LAD-code state the type of district or LAD prefix. There are for different types of districts or LAD prefixes:\n\nE06 = Unitary Authority: mixture of urban and rural areas.\nE07 = Non-Metropolitan District: rural areas or small towns.\nE08 = Metropolitan District: large towns and cities.\nE09 = London Borough: rural and small towns.\n\nIt seems reasonable to expect that the average percentage of residents that visited a library varies accross district types. More precisely, I would expect that the more “urban/metropolitan” a district is, the higher the percentage of residents that visited a library. Conversely, the more “rural” a district is, the lower the percentage of residents that visited a library. According to this logic, I would expect that the percentage of residents that visited a library is highest for the districts belonging to Greater London (E09), followed by metropolitan districts (E08), Unitary Authorities (E06) as mixture of urban and rural areas and finally the purely rural districts (E06).\nTo check this, I assign each district its district type (i.e. LAD prefix) and then compute the mean percentage of residents that visited a library per district type as well as the corresponding standard error around the mean.\n\nlibrary(dplyr)      # for data manipulation\nlibrary(ggplot2)    # for plotting\n\n# assign district type\nusage_df$LAD_prefix &lt;- factor(substr(usage_df$LAD_code, 1, 3),\n                             levels = c(\"E09\",\"E08\",\"E07\",\"E06\"))\n\np1_df &lt;- usage_df %&gt;%\n  group_by(LAD_prefix) %&gt;%\n  summarise(\n    mean_visits = mean(visits, na.rm=TRUE),\n    sd_visits = sd(visits, na.rm=TRUE),\n    n = sum(!is.na(visits)),\n    se = sd_visits/sqrt(n)\n  )\n\np1 &lt;- ggplot(p1_df, aes(LAD_prefix, mean_visits)) +\n  geom_col(width = 0.5, fill=\"#0b0b64\") +\n  geom_errorbar(aes(ymin=mean_visits - se, ymax=mean_visits + se),\n                width = 0.2) +\n  geom_text(aes(label=paste0(\"n=\", n)), vjust=-1.2, size=3.5) +\n  labs(\n    title = \"Library Usage per District Type\",\n    x = \"LAD Prefix\",\n    y = \"Mean %-age of Residents Visiting a Library\"\n  ) +\n  theme_bw()\n\np1\n\n\n\n\n\n\n\n\nAs expected, the districts belonging to Greater London (E09) have the highest percentage of residents that visited a library. Two things are however astonishing: On the one hand, the metropolitan districts (E08), which I expected to rank second, rank lowest. On the other hand, the rural district (E07), which I expected to rank lowest, in fact rank second. This is all the more remarkable as for the rural districts the accessibility of libraries is worst, as can be seen from the plot below.\n\n\nShow code\naccess_df$LAD_prefix &lt;- factor(substr(access_df$LAD_code, 1, 3),\n                             levels = c(\"E09\",\"E08\",\"E07\",\"E06\"))\n\naccess_df &lt;- access_df[!is.na(access_df$LAD_prefix),]\n\np2_df &lt;- access_df %&gt;%\n  group_by(LAD_prefix) %&gt;%\n  summarise(\n    mean_proximity = mean(proximity, na.rm=TRUE),\n    sd_proximity = sd(proximity, na.rm=TRUE),\n    n = sum(!is.na(proximity)),\n    se = sd_proximity/sqrt(n)\n  )\n\np2 &lt;- ggplot(p2_df, aes(LAD_prefix, mean_proximity)) +\n  geom_col(width = 0.5, fill=\"#0b0b64\") +\n  geom_errorbar(aes(ymin=mean_proximity - se, ymax=mean_proximity + se),\n                width = 0.2) +\n  geom_text(aes(label=paste0(\"n=\", n)), vjust=-1.2, size=3.5) +\n  ylim(0,105) +\n  labs(\n    title = \"Library Accessibility per District Type\",\n    x = \"LAD Prefix\",\n    y = \"Mean %-age of Residents within a 3km Distance from a Library\"\n  ) +\n  theme_bw()\n\np2\n\n\n\n\n\n\n\n\n\n\n\nSpatial Analysis of Library Usage\nNow, let’s have a look at spatial differences in the usage of libraries. More specifically, I want to plot a map of England with each district coloured according to its percentage of residents that visited a library. In order to plot the districts on a map, I need for every district information about its geographical boundaries. This information can be obtained from ONS’s Open Geography Portal.\n\nlibrary(sf)     # for handling spatial data\n\n# get geographical boundaries\nurl &lt;- \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/LAD_MAY_2025_UK_BUC/FeatureServer/0/query?outFields=*&where=1%3D1&f=geojson\"\nboundaries &lt;- read_sf(url)\n\nusage_df &lt;- left_join(boundaries, usage_df, by = c(\"LAD25CD\"=\"LAD_code\"))\n\nThe boundaries dataframe contains for each district a description of its territory as a polygon. Next, I join the boundaries data and the usage data using a left join. The resulting data set is the basis for plotting a map of England with each district coloured according to the percentag of residents that visited a library.\n\np3_df &lt;- usage_df %&gt;%\n  mutate(\n    category = cut(\n      visits,\n      breaks = c(12, 19, 23, 27, 31, 38),\n      labels = c(\n        \"12–19%\", \"19–23%\", \"23–27%\", \"27–31%\", \"31–38%\"\n      ),\n      include.lowest = TRUE,\n      right = FALSE\n    )\n  )\n\np3_df$category &lt;- factor(p3_df$category, \n                         levels=\n                           c(\"31–38%\", \"27–31%\", \"23–27%\", \"19–23%\", \"12–19%\")\n                         )\n\np3 &lt;- ggplot(p3_df, aes(fill = category)) +\n  geom_sf(color = \"white\", size = 0.1) +\n  scale_fill_manual(\n    name = \"%-age of Residents\\nVisiting a Library\",\n    values = c(\n      \"12–19%\" = \"#8B0000\",\n      \"19–23%\" = \"#E74C3C\",\n      \"23–27%\" = \"#A8E6A3\",\n      \"27–31%\" = \"#34A853\",\n      \"31–38%\" = \"#006400\"\n    ),\n    na.value=\"lightgrey\",\n    drop = FALSE\n  ) +\n  coord_sf(ylim = c(50,58.5)) +\n  theme_bw() +\n  theme(\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 9)\n  )\n\np3\n\n\n\n\n\n\n\n\nLooking at the above plot, there seems to be some degree of positive spatial dependence. That is the neighbours of districs with high percentages of residents visiting a library also have high percentages (and vice versa). As a result there seem to be clusters with either all districts having higher or all having lower percentage of residents visiting libraries. If there was a negative spatial dependence, the neighbours of districts with high percentages would have low percentages (and vice versa).\nInstead of relying only on visual inspection, I rather want to see if this impression can be substantiated by some quantitative measure. Such a measure for quantifying spatial dependence, or more precisely spatial autocorrelation, is Global Moran’s \\(I\\) (Moraga 2023). It is defined as: \\[ I = \\frac{n}{W} \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij} (x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\] where \\(n\\) is the number of districts, \\(x_i\\) is the observed value of the variable of interest in district \\(i=1,\\dots, n\\) and \\(\\bar{x}\\) is the mean of all \\(x_i\\)’s. The \\(w_{ij}\\)’s are weights that measure the spatial proximity between districts \\(i\\) and \\(j\\) (with \\(w_{ii}=0\\)) and \\(W\\) is the sum of all weights, i.e. \\(W=\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}\\). The exact definition of proximity and hence also of \\(w_{ij}\\) depends on the application at hand. In this application here proximity means that two districts are direct neighbours. Consequently, the weights are defined as: \\[ w_{ij} =\n\\begin{cases}\n1, & \\text{if district } i \\text{ and } j \\text{ are direct neighbours}, \\\\\n0, & \\text{else}.\n\\end{cases}\\] Intuitively, \\(I\\) is the correlation between a district’s value and the weighted average of its neighbour’s values. The interpretation of Moran’s \\(I\\) is identical to the interpretation of the Pearson coefficient of correlation (\\(-1\\) = perfect negative association, \\(0\\) = no association, \\(+1\\) = perfect positive association).\nTo compute Moran’s \\(I\\), the following three steps need to be performed:\n\nFor each district \\(i\\), determine the districts \\(j\\) that are direct neighbours of \\(i\\).\nFor each district \\(i\\), assign a weight of \\(1\\) to all its direct neighbours and a weight of \\(0\\) to all other districts.\nUse those weights, to compute \\(I\\).\n\nThese three steps can be performed utilizing the spatial dependence R package (spdep) as follows:\n\nlibrary(spdep)\nsf_use_s2(FALSE)\n\nusage_df &lt;- usage_df[!is.na(usage_df$visits), ]\n\n# 1. determine direct neighbours (as queen in chess can move)\nneighbours &lt;- poly2nb(usage_df, queen=TRUE)\n\n# 2. assign weights, i.e. from list of neighbours compute list of weights\nweights &lt;- nb2listw(neighbours, style=\"B\", zero.policy=TRUE)\n\n# 3. compute Moran's I\n(I &lt;- moran(usage_df$visits, weights, length(neighbours), Szero(weights))[[1]])\n\n[1] 0.3171098\n\n\nA Moran’s \\(I\\) of 0.32 means that there is a moderate degree of positive spatial dependence, which supports the impression I had from the visual inspection. Finally, I want to test whether Moran’s \\(I\\) is statistically significantly positive. So the null hypothesis is that \\(I\\) is less than or equal to \\(O\\), whereas the alternative hypothesis is that \\(I\\) is greater than \\(0\\). This can be done using a Monte-Carlo based approach.\n\nmoran.mc(usage_df$visits, weights, nsim=999, alternative = \"greater\")\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  usage_df$visits \nweights: weights  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.31495, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nThe result of the hypothesis test suggests that there is significant positive spatial autocorrelation, i.e., nearby districts have similar percentage of residents visiting a library.\n\n\nConclusion\nIn summary, there are two insights: First, library usage differs accross district type. Surprisingly, it is the rural districts rather than the metropolitan districts that rank second in library usage after the districts belonging to Greater London. Second, there is positive spatial dependence in library usage between neighbouring districts.\n\n\n\n\n\nReferences\n\nMoraga, Paula. 2023. Spatial Statistics for Data Science: Theory and Practice with r. Chapman; Hall/CRC."
  },
  {
    "objectID": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "href": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html",
    "title": "Neural Networks for Solving Ordinary Differential Equations",
    "section": "",
    "text": "In supervised learning settings, neural networks are employed to estimate conditional expectations or conditional probabilities. Some while ago, I read of a completely different application of neural networks. In fact, neural networks, can be used to numerically approximate the solutions of (ordinary) differential equations (Blechschmidt and Ernst 2021). Before implementing it from scratch in Python, I will first describe the setting and the general approach."
  },
  {
    "objectID": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-neural-network",
    "href": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#define-neural-network",
    "title": "Neural Networks for Solving Ordinary Differential Equations",
    "section": "Define Neural Network",
    "text": "Define Neural Network\nFirst, I define the neural network used for the approximation. It is a feed forward multilayer perceptron with one hidden layer that applies a \\(sigmoid\\) activation function between two fully connected linear layers.\n\nclass N(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size):\n    super(N, self).__init__()\n    self.net = nn.Sequential(\n      nn.Linear(input_size, hidden_size),\n      nn.Sigmoid(),\n      nn.Linear(hidden_size, output_size)\n      )\n    \n  def forward(self, x):\n    return self.net(x)"
  },
  {
    "objectID": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#first-ode",
    "href": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#first-ode",
    "title": "Neural Networks for Solving Ordinary Differential Equations",
    "section": "First ODE",
    "text": "First ODE\nThe first example is an IVP consisting of a second order ODE given by: \\[ 4 u(x) + u^{(1)}(x) + u^{(2)}(x) = 0 \\] and initial conditions \\[ u(0) = 0.5, \\; u^{(1)}(0)=2 .\\]\n\nDefine Custom Loss Function\nThe custom loss function corresponding to the above IVP is computed as:\n\nclass Loss_IVP1(nn.Module):\n  def __init__(self):\n    super(Loss_IVP1, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    u = model(x)\n    \n    # compute first derivative\n    du_dx = torch.autograd.grad(\n      outputs=u,\n      inputs=x,\n      grad_outputs=torch.ones_like(u),\n      create_graph=True\n      )[0]\n    \n    # compute second derivative\n    d2u_dx2 = torch.autograd.grad(\n      outputs=du_dx,\n      inputs=x,\n      grad_outputs=torch.ones_like(du_dx),\n      create_graph=True)[0]\n    \n    # compute loss function\n    loss_ODE = torch.mean((4*u + du_dx + d2u_dx2)**2)\n    loss_IC = (u[0] - 0.5)**2 + (du_dx[0] - 2)**2\n    loss_total = loss_ODE + loss_IC\n    \n    return loss_total\n\n\n\nConstruct Model, Loss and Optimizer\nTo ensure reproducibility of the subsequent steps, it is advisable to set a seed for the internal random number generator.\n\nseed = 42\ntorch.manual_seed(seed)\n\n&lt;torch._C.Generator at 0x2dac1e27130&gt;\n\n\nI instantiate the neural network, the loss function to be minimized and the optimizer (here, I use Adam).\n\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss_IVP1()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSet x-values\nI decided to approximate the solution of the IVP on the interval \\(I=[0,5]\\). I choose \\(n=100\\) sample points evenly spread accross \\(I\\).\n\nx = torch.linspace(0, 5, 100)[:, None]\n\n\n\nTraining\nNow that all necessary components are set up, training can start.\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.1181\nEpoch [2/10], Loss: 0.0307\nEpoch [3/10], Loss: 0.0052\nEpoch [4/10], Loss: 0.0032\nEpoch [5/10], Loss: 0.0008\nEpoch [6/10], Loss: 0.0002\nEpoch [7/10], Loss: 0.0001\nEpoch [8/10], Loss: 0.0000\nEpoch [9/10], Loss: 0.0000\nEpoch [10/10], Loss: 0.0000\n\n\n\n\nVisualization\nOnce training is done, the learned, approximate solution can be compared to the analytical solution, which is known for this IVP. The analytical solution is given by: \\[ u(x) = e^{-0.5x} \\times \\left[ 0.5 \\times \\cos\\left(\\frac{\\sqrt{15}}{2} x \\right) + \\frac{3 \\sqrt{15}}{10} \\times \\sin \\left( \\frac{\\sqrt{15}}{2} x \\right) \\right] \\]\n\n\nShow code\nx = torch.linspace(0, 15, 1000)[:, None]\nwith torch.no_grad():\n  u = model(x)\n\nplt.figure(figsize=(6, 5))\nplt.plot(x, u, color=\"C3\", label=\"Neural Network\")\nplt.plot(x, torch.exp(-0.5*x)*(0.5*torch.cos(x*(15)**0.5/2)+3*((15)**0.5/10)*torch.sin(x*(15)**0.5/2)), color=\"C0\", label=\"Analytical\", linestyle=\"dotted\")\nplt.title(\"Solution of the IVP\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe above plot shows the approximate as well as the analytical solution. Apparently, on the chosen interval \\(I=[0,5]\\) the approximate solution matches the analytical solution. Outside of \\(I\\), the approximate solution deviates from the analytical one."
  },
  {
    "objectID": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#second-ode",
    "href": "coding/SolvingODEsUsingANNs/SolvingODEsUsingANNs.html#second-ode",
    "title": "Neural Networks for Solving Ordinary Differential Equations",
    "section": "Second ODE",
    "text": "Second ODE\nThe second example is an IVP that consists of the following first ODE \\[ u^{(1)}(x) - 2x (2-u) = 0 \\] and the initial condition \\[ u(0) = -1 .\\]\n\nDefine Custom Loss Function\nFor this IVP, the custom loss function is computed as:\n\nclass Loss_IVP2(nn.Module):\n  def __init__(self):\n    super(Loss_IVP2, self).__init__()\n  \n  def forward(self, model, x):\n    x = x.clone().detach().requires_grad_(True)\n    u = model(x)\n    \n    # compute first derivative\n    du_dx = torch.autograd.grad(\n      outputs=u,\n      inputs=x,\n      grad_outputs=torch.ones_like(u),\n      create_graph=True\n      )[0]\n    \n    # compute loss function\n    loss_DE = torch.mean((du_dx - 2*x*(2-u))**2)\n    loss_initial = (u[0] + 1)**2\n    loss_total = loss_DE + loss_initial\n    \n    return loss_total\n\n\n\nConstruct Model, Loss and Optimizer\nAgain, a seed is set for the internal random number generator.\n\nseed = 42\ntorch.manual_seed(seed)\n\n&lt;torch._C.Generator at 0x2dac1e27130&gt;\n\n\nThe neural network, the loss function to be minimized and the optimizer are instantiated.\n\nmodel = N(input_size=1, hidden_size=10, output_size=1)\ncriterion = Loss_IVP2()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\n\n\nSet x-values\nAs previously, the solution of the IVP is approximated on the interval \\(I=[0,5]\\) with \\(n=100\\) sample points evenly spread accross \\(I\\).\n\nx = torch.linspace(0, 5, 100)[:, None]\n\n\n\nTraining\nThe setup is finished and the training can begin.\n\nnum_epochs = 10\nsteps_per_epoch = 3000\n\nfor epoch in range(num_epochs):\n  for step in range(steps_per_epoch):\n    loss = criterion(model, x)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n  print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\nEpoch [1/10], Loss: 0.3274\nEpoch [2/10], Loss: 0.0380\nEpoch [3/10], Loss: 0.0141\nEpoch [4/10], Loss: 0.0080\nEpoch [5/10], Loss: 0.0025\nEpoch [6/10], Loss: 0.0003\nEpoch [7/10], Loss: 0.0000\nEpoch [8/10], Loss: 0.0000\nEpoch [9/10], Loss: 0.0000\nEpoch [10/10], Loss: 0.0000\n\n\n\n\nVisualization\nFor this IVP, the analytical solution is given by: \\[ u(x) = 2 - 3 \\, e^{-x^2}. \\]\n\n\nShow code\nx = torch.linspace(-5, 5, 1000)[:, None]\nwith torch.no_grad():\n  u = model(x)\n\nplt.figure(figsize=(6, 5))\nplt.plot(x, u, color=\"C3\", label=\"Neural Network\")\nplt.plot(x, 2 - 3*torch.exp(-x**2), color=\"C0\" , label=\"Analytical\", linestyle=\"dotted\")\nplt.title(\"Solution of the IVP\")\nplt.xlabel(\"x\")\nplt.ylabel(\"u\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\nAlso for the second exemplary IVP, on the interval \\(I=[0,5]\\) the approximate solution closely aligns with the analytical solution, whereas outside of it both deviate."
  }
]